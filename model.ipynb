{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02baae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x32dfa73e0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "\n",
    "# Profiler\n",
    "import cProfile\n",
    "import re\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1cfb132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, critic_learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.critic_learning_rate)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.critic(input)\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, actor_learning_rate, std, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.std = std\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.out_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.actor_learning_rate)\n",
    "\n",
    "    def forward(self, in_):\n",
    "\n",
    "        if in_.dim() == 1:\n",
    "            in_ = in_.unsqueeze(0)\n",
    "        h = self.actor(in_)\n",
    "\n",
    "        epsilon = torch.randn(h.size(0), h.size(1)).to(self.device)\n",
    "        z = h + self.std * epsilon\n",
    "        return z, h, self.std\n",
    "    \n",
    "    def get_log_probability(self, z, mu, std):\n",
    "\n",
    "        coeff =  1 / (std*math.sqrt(2*math.pi))\n",
    "        normal_dist = coeff * torch.exp(-0.5 * (((z - mu) / std) ** 2) )\n",
    "        assert(normal_dist.all() >= 0)\n",
    "\n",
    "        return torch.log(normal_dist).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd7dc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, epochs, training_iterations, batch_size, trajectory_length, n_actors, env, in_features, out_features, hidden_features, device, actor_learning_rate, critic_learning_rate, gamma, lambda_, epsilon, std, beta, d_targ, mode):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.training_iterations = training_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.n_actors = n_actors\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        self.d_targ = d_targ\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.std = std\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.mode = mode\n",
    "\n",
    "        self.actor = Policy(in_features, out_features, hidden_features, actor_learning_rate, std, device)\n",
    "        self.critic = ValueFunction(in_features, out_features, hidden_features, critic_learning_rate)\n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "\n",
    "        N = self.n_actors  #number of actors\n",
    "        T = self.trajectory_length # trajectory length\n",
    "        \n",
    "        for i in range(self.training_iterations):\n",
    "            dataset = []\n",
    "            print(f\"[train]: starting dataset creation at iteration n {i}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                adv_list = []\n",
    "                cum_reward = 0\n",
    "                for _ in range(N): #for each actor\n",
    "\n",
    "                    # initialize first state\n",
    "                    s_prime, _ = self.env.reset()\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    trajectory = []\n",
    "                    done = False\n",
    "\n",
    "                    for t in range(T):\n",
    "                        action, mu, std = self.actor(s_prime)\n",
    "                        log_policy = self.actor.get_log_probability(action, mu, std)\n",
    "\n",
    "                        s, reward, terminated, truncated, _ = self.env.step(action.squeeze(0).cpu().detach().numpy())\n",
    "                        s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                        reward = torch.tensor([[reward]], dtype=torch.float32).to(self.device)\n",
    "                        s_prime = s_prime.unsqueeze(0)\n",
    "                        trajectory.append([s_prime, action, reward, log_policy])\n",
    "                        s_prime = s\n",
    "                        cum_reward += reward\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "\n",
    "                    dynamic_target = 0 if done else self.critic(s)\n",
    "                    for t in range(len(trajectory)-1, -1, -1): #I want the range from [T-1 to 0]\n",
    "                        \n",
    "                        dynamic_target = dynamic_target*self.gamma + trajectory[t][2] #taking the reward\n",
    "                        advantage = dynamic_target - self.critic(trajectory[t][0])\n",
    "                        trajectory[t] = tuple(trajectory[t] + [dynamic_target.unsqueeze(0), advantage.unsqueeze(0)])\n",
    "\n",
    "                        dataset.append(trajectory[t])\n",
    "                        adv_list.append(advantage)\n",
    "\n",
    "                adv_std, adv_mean = torch.std_mean(torch.tensor(adv_list))\n",
    "                print(f\"[training]: cum reward {cum_reward}\")\n",
    "            \n",
    "            print(f\"[training]: ending dataset creation with dataset size {len(dataset)}\")\n",
    "\n",
    "            self.actor.zero_grad()\n",
    "            self.critic.zero_grad()\n",
    "            # Starts the training process\n",
    "            for e in range(self.epochs):\n",
    "                \n",
    "                print(f\"[train]: epoch n {e}\")\n",
    "                avg_loss_value = 0\n",
    "                avg_loss_ppo = 0\n",
    "                rd.shuffle(dataset) #shuffle in-place\n",
    "                \n",
    "                assert(self.batch_size <= len(dataset))\n",
    "\n",
    "                for mini_idx in range(0, len(dataset), self.batch_size):\n",
    "                    \n",
    "                    # form mini_batch\n",
    "                    mini_batch = dataset[mini_idx: mini_idx+self.batch_size]\n",
    "\n",
    "                    state_mini = torch.stack(list(map(lambda elem: elem[0].squeeze(), mini_batch)))\n",
    "                    action_mini = torch.stack(list(map(lambda elem: elem[1].squeeze(), mini_batch)))\n",
    "                    log_policy_mini = torch.stack(list(map(lambda elem: elem[3].squeeze(), mini_batch)))\n",
    "                    advantage_mini = torch.stack(list(map(lambda elem: elem[4].squeeze(), mini_batch)))\n",
    "                    target_mini = torch.stack(list(map(lambda elem: elem[5].squeeze(), mini_batch)))\n",
    "                    \n",
    "                    # Normalize advantage_mini\n",
    "                    advantage_mini = ((advantage_mini-adv_mean) / (adv_std+0.00001))\n",
    "\n",
    "                    _, mu_mini, std_mini = self.actor(state_mini) # std is a scalar!\n",
    "                    new_log_policy_mini = self.actor.get_log_probability(action_mini, mu_mini, std_mini)   \n",
    "\n",
    "                    new_value_mini = self.critic(state_mini)\n",
    "                    \n",
    "                    self.actor.optimizer.zero_grad()\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "                    \n",
    "                    if self.mode == 'clip':\n",
    "                        loss_ppo = self.loss_clip(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "                    elif (self.mode == 'kl_fixed') or (self.mode == 'kl_adaptive'):\n",
    "                        loss_ppo = self.loss_kl(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "\n",
    "                    loss_value = self.loss_value(new_value_mini, target_mini)\n",
    "\n",
    "                    avg_loss_ppo += loss_ppo\n",
    "                    avg_loss_value += avg_loss_value\n",
    "\n",
    "                    loss_ppo.backward()\n",
    "                    loss_value.backward()\n",
    "                    \n",
    "                    self.actor.optimizer.step()\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "\n",
    "                total_minibatch = math.floor(len(dataset) // self.batch_size)\n",
    "                print(f\"[avg actor loss]: {avg_loss_ppo / total_minibatch} \\t[critic loss]: {loss_value / total_minibatch}\")\n",
    "\n",
    "\n",
    "            self.save_parameters(\"model\"+str(i)+\".pt\")\n",
    "                \n",
    "\n",
    "    def loss_value(self, value, target):\n",
    "        #MSE\n",
    "        return torch.mean((value-target)**2)\n",
    "\n",
    "    def loss_clip(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "\n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini*advantage_mini\n",
    "        clip_ = torch.clip(prob_mini, 1-self.epsilon, 1+self.epsilon)*advantage_mini\n",
    "        return -torch.min(prob_adv, clip_).mean()\n",
    "    \n",
    "    def loss_kl(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "        \n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini * advantage_mini\n",
    "        d = log_policy_mini - new_log_policy_mini\n",
    "\n",
    "        if self.mode == 'kl_adaptive':\n",
    "            if d.detach().mean() < (self.d_targ / 1.5):\n",
    "                self.beta = self.beta / 2\n",
    "            elif d.detach().mean() > (self.d_targ * 1.5):\n",
    "                self.beta = self.beta * 2\n",
    "\n",
    "        return -(prob_adv - self.beta*d).mean()\n",
    "        \n",
    "\n",
    "    def extract_states_prime(self, trajectory):\n",
    "        return list(map(lambda x: x[0], trajectory))\n",
    "    \n",
    "    def save_parameters(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1)\n",
    "\n",
    "epochs = 10\n",
    "training_iterations = 20\n",
    "batch_size = 64\n",
    "trajectory_length = 500\n",
    "n_actors = 10\n",
    "in_features = env.observation_space.shape[0]\n",
    "out_features = env.action_space.shape[0]\n",
    "hidden_features = 64\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-4\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "epsilon = 0.2\n",
    "beta = 1\n",
    "d_targ = 0.01\n",
    "std = 0.5\n",
    "device = \"mps\"\n",
    "mode = \"kl_adaptive\"\n",
    "modes = [\"kl_fixed\", \"kl_adaptive\", \"clip\", \"pc\"]\n",
    "assert(mode in modes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f3dabe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (actor): Policy(\n",
       "    (actor): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (critic): ValueFunction(\n",
       "    (critic): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo = PPO(epochs=epochs, \n",
    "          training_iterations=training_iterations,\n",
    "          batch_size=batch_size,\n",
    "          trajectory_length=trajectory_length, \n",
    "          n_actors=n_actors,\n",
    "          env=env,\n",
    "          in_features=in_features,\n",
    "          out_features=out_features,\n",
    "          hidden_features=hidden_features,\n",
    "          device=device,\n",
    "          actor_learning_rate=actor_learning_rate,\n",
    "          critic_learning_rate=critic_learning_rate,\n",
    "          gamma=gamma,\n",
    "          lambda_=lambda_,\n",
    "          epsilon=epsilon,\n",
    "          beta = beta,\n",
    "          d_targ=d_targ,\n",
    "          std=std,\n",
    "          mode = mode,\n",
    "        )\n",
    "ppo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329654a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: starting dataset creation at iteration n 0\n",
      "[training]: cum reward tensor([[-1677.2737]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -0.008941580541431904 \t[critic loss]: 15.614160537719727\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: 0.39657339453697205 \t[critic loss]: 9.408950805664062\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.009127650409936905 \t[critic loss]: 20.85405731201172\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.018223851919174194 \t[critic loss]: 15.105571746826172\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.022643450647592545 \t[critic loss]: 11.20498275756836\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.022687431424856186 \t[critic loss]: 8.253515243530273\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.013345897197723389 \t[critic loss]: 25.495573043823242\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.022337574511766434 \t[critic loss]: 9.878246307373047\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.021400855854153633 \t[critic loss]: 14.406143188476562\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.02144877053797245 \t[critic loss]: 14.357537269592285\n",
      "[train]: starting dataset creation at iteration n 1\n",
      "[training]: cum reward tensor([[-1875.8589]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.001679010922089219 \t[critic loss]: 16.684301376342773\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.004562455229461193 \t[critic loss]: 12.222993850708008\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.001505415071733296 \t[critic loss]: 20.016830444335938\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.0013648944441229105 \t[critic loss]: 24.01028823852539\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.01148699689656496 \t[critic loss]: 14.141408920288086\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.017705703154206276 \t[critic loss]: 10.621782302856445\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.010390806943178177 \t[critic loss]: 22.178466796875\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.01816902682185173 \t[critic loss]: 18.714038848876953\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.020832471549510956 \t[critic loss]: 20.929256439208984\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.03302885964512825 \t[critic loss]: 9.422554016113281\n",
      "[train]: starting dataset creation at iteration n 2\n",
      "[training]: cum reward tensor([[-1501.8136]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.0018252467270940542 \t[critic loss]: 11.247509956359863\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: 0.0013355024857446551 \t[critic loss]: 18.95894432067871\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.006559548433870077 \t[critic loss]: 10.564152717590332\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.01445099525153637 \t[critic loss]: 7.246041297912598\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.017724093049764633 \t[critic loss]: 7.021460056304932\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.021901827305555344 \t[critic loss]: 8.234865188598633\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.02830011025071144 \t[critic loss]: 6.786520957946777\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.03077760711312294 \t[critic loss]: 9.21603012084961\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.038840871304273605 \t[critic loss]: 10.993146896362305\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.03293199837207794 \t[critic loss]: 23.49832534790039\n",
      "[train]: starting dataset creation at iteration n 3\n",
      "[training]: cum reward tensor([[-1770.8475]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -0.007972626015543938 \t[critic loss]: 6.237654685974121\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.011827830225229263 \t[critic loss]: 5.999958515167236\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.012700166553258896 \t[critic loss]: 8.843595504760742\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.01303030550479889 \t[critic loss]: 13.77242374420166\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.014199702069163322 \t[critic loss]: 18.43990135192871\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.025249449536204338 \t[critic loss]: 14.154189109802246\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.03429298475384712 \t[critic loss]: 11.24618148803711\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.034906771034002304 \t[critic loss]: 25.595722198486328\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.06625735759735107 \t[critic loss]: 11.16404914855957\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.104534812271595 \t[critic loss]: 15.782536506652832\n",
      "[train]: starting dataset creation at iteration n 4\n",
      "[training]: cum reward tensor([[-554.4384]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.0011403855169191957 \t[critic loss]: 1.829955816268921\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.00465443916618824 \t[critic loss]: 1.9750592708587646\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.013971781358122826 \t[critic loss]: 2.4714910984039307\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.018022237345576286 \t[critic loss]: 1.2432467937469482\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.011687066406011581 \t[critic loss]: 5.142577648162842\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.03057020530104637 \t[critic loss]: 2.023128032684326\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: 5118.26416015625 \t[critic loss]: 4.067105770111084\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: 112548913152.0 \t[critic loss]: 1.9093767404556274\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: 2147968876544.0 \t[critic loss]: 2.209317207336426\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: 946964791296.0 \t[critic loss]: 2.1686105728149414\n",
      "[train]: starting dataset creation at iteration n 5\n",
      "[training]: cum reward tensor([[-929.3652]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -1565.95068359375 \t[critic loss]: 3.661959171295166\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: 0.0033299517817795277 \t[critic loss]: 6.053698539733887\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: 0.00012340240937191993 \t[critic loss]: 3.8582520484924316\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: 0.0072029875591397285 \t[critic loss]: 7.971042156219482\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.007199190557003021 \t[critic loss]: 1.780415415763855\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: 0.0024372609332203865 \t[critic loss]: 4.996543884277344\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.0063009439036250114 \t[critic loss]: 2.6841211318969727\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: 0.00572097348049283 \t[critic loss]: 7.87838888168335\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: 0.007585326675325632 \t[critic loss]: 8.793134689331055\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.002544465009123087 \t[critic loss]: 2.4912633895874023\n",
      "[train]: starting dataset creation at iteration n 6\n",
      "[training]: cum reward tensor([[-857.5818]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -0.002561799716204405 \t[critic loss]: 1.9890543222427368\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.0028881262987852097 \t[critic loss]: 3.392486095428467\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.003378921654075384 \t[critic loss]: 2.711092710494995\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.000972180045209825 \t[critic loss]: 2.8964545726776123\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.0020509420428425074 \t[critic loss]: 2.259612560272217\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.004360423423349857 \t[critic loss]: 2.976362466812134\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: 0.002007919130846858 \t[critic loss]: 4.21763801574707\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.005186344496905804 \t[critic loss]: 3.347081422805786\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.0023415859322994947 \t[critic loss]: 2.9263198375701904\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.0017401365330442786 \t[critic loss]: 3.178544044494629\n",
      "[train]: starting dataset creation at iteration n 7\n",
      "[training]: cum reward tensor([[-682.7788]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.0056432392448186874 \t[critic loss]: 6.537064552307129\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.0030459119006991386 \t[critic loss]: 3.884486198425293\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: 0.003567761043086648 \t[critic loss]: 4.573010444641113\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.007600139360874891 \t[critic loss]: 2.096565008163452\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: 0.0010589798912405968 \t[critic loss]: 3.7755677700042725\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.0023196921683847904 \t[critic loss]: 4.099820613861084\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.005120910704135895 \t[critic loss]: 1.528111219406128\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.0023691400419920683 \t[critic loss]: 3.6504108905792236\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: 0.00022589463333133608 \t[critic loss]: 4.2608962059021\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.002229739911854267 \t[critic loss]: 2.709747791290283\n",
      "[train]: starting dataset creation at iteration n 8\n",
      "[training]: cum reward tensor([[-850.9523]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -0.0056557366624474525 \t[critic loss]: 1.8415693044662476\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.003165739355608821 \t[critic loss]: 2.0237491130828857\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: 0.005969524383544922 \t[critic loss]: 6.698744773864746\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: 0.000706141407135874 \t[critic loss]: 6.579679012298584\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: 0.0025852550752460957 \t[critic loss]: 4.604982376098633\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.004526650998741388 \t[critic loss]: 2.6343994140625\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: 0.0008862382383085787 \t[critic loss]: 4.987885475158691\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: 0.0017475929344072938 \t[critic loss]: 5.686503887176514\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.0003735993814188987 \t[critic loss]: 3.208157777786255\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: 0.0025230299215763807 \t[critic loss]: 4.984159469604492\n",
      "[train]: starting dataset creation at iteration n 9\n",
      "[training]: cum reward tensor([[-811.9341]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.003355535911396146 \t[critic loss]: 4.374450206756592\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: 0.0036509381607174873 \t[critic loss]: 6.945759296417236\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.007137681357562542 \t[critic loss]: 1.6267485618591309\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: -0.007907610386610031 \t[critic loss]: 2.3065385818481445\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.0038760609459131956 \t[critic loss]: 2.306035280227661\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: -0.0010547038400545716 \t[critic loss]: 3.8956165313720703\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -0.001982845598831773 \t[critic loss]: 2.607095956802368\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.0009698046487756073 \t[critic loss]: 3.5764873027801514\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: 0.0007251763599924743 \t[critic loss]: 3.596095085144043\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: -0.0015149236423894763 \t[critic loss]: 3.397207498550415\n",
      "[train]: starting dataset creation at iteration n 10\n",
      "[training]: cum reward tensor([[-1138.6887]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.007685883902013302 \t[critic loss]: 12.17617130279541\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.0033521349541842937 \t[critic loss]: 6.42030143737793\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.0020593740046024323 \t[critic loss]: 5.617007732391357\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: 0.0008177919662557542 \t[critic loss]: 6.859968185424805\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: -0.007609599735587835 \t[critic loss]: 2.771242618560791\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: 0.002359997248277068 \t[critic loss]: 8.483086585998535\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: -9.733401384437457e-05 \t[critic loss]: 6.076183319091797\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: -0.00042148580541834235 \t[critic loss]: 6.022282123565674\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: 0.0028556922916322947 \t[critic loss]: 9.580560684204102\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: 0.002433305373415351 \t[critic loss]: 8.558053016662598\n",
      "[train]: starting dataset creation at iteration n 11\n",
      "[training]: cum reward tensor([[-968.3331]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: 0.0033459074329584837 \t[critic loss]: 6.839421272277832\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: -0.0005245779757387936 \t[critic loss]: 4.314643859863281\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.00505561800673604 \t[critic loss]: 2.0319905281066895\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: 0.0028114255983382463 \t[critic loss]: 6.099946022033691\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: 0.003130852011963725 \t[critic loss]: 5.02015495300293\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: 0.003263734746724367 \t[critic loss]: 6.997308254241943\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: 0.000270177552010864 \t[critic loss]: 4.066298961639404\n",
      "[train]: epoch n 7\n",
      "[avg actor loss]: 0.0008995968382805586 \t[critic loss]: 5.091560363769531\n",
      "[train]: epoch n 8\n",
      "[avg actor loss]: -0.0027076273690909147 \t[critic loss]: 3.462040901184082\n",
      "[train]: epoch n 9\n",
      "[avg actor loss]: 0.00044177912059240043 \t[critic loss]: 5.917270660400391\n",
      "[train]: starting dataset creation at iteration n 12\n",
      "[training]: cum reward tensor([[-826.1785]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "[avg actor loss]: -0.002847616793587804 \t[critic loss]: 2.868915319442749\n",
      "[train]: epoch n 1\n",
      "[avg actor loss]: 0.004176816903054714 \t[critic loss]: 4.921237945556641\n",
      "[train]: epoch n 2\n",
      "[avg actor loss]: -0.00027473835507407784 \t[critic loss]: 4.29268741607666\n",
      "[train]: epoch n 3\n",
      "[avg actor loss]: 0.00033501783036626875 \t[critic loss]: 2.8508453369140625\n",
      "[train]: epoch n 4\n",
      "[avg actor loss]: 0.01090278197079897 \t[critic loss]: 9.984440803527832\n",
      "[train]: epoch n 5\n",
      "[avg actor loss]: 0.0013980390504002571 \t[critic loss]: 4.264980316162109\n",
      "[train]: epoch n 6\n",
      "[avg actor loss]: 0.0007789707742631435 \t[critic loss]: 3.7522644996643066\n",
      "[train]: epoch n 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#ppo.load_state_dict(torch.load(\"final.pt\"))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mppo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ppo.save_parameters(\u001b[33m\"\u001b[39m\u001b[33mfinal_adaptive.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mPPO.train_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    116\u001b[39m     loss_ppo = \u001b[38;5;28mself\u001b[39m.loss_clip(new_log_policy_mini, log_policy_mini, advantage_mini)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mkl_fixed\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mkl_adaptive\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     loss_ppo = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_kl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_log_policy_mini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_policy_mini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantage_mini\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m loss_value = \u001b[38;5;28mself\u001b[39m.loss_value(new_value_mini, target_mini)\n\u001b[32m    122\u001b[39m avg_loss_ppo += loss_ppo\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mPPO.loss_kl\u001b[39m\u001b[34m(self, new_log_policy_mini, log_policy_mini, advantage_mini)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mkl_adaptive\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m d.detach().mean() < (\u001b[38;5;28mself\u001b[39m.d_targ / \u001b[32m1.5\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m = \u001b[38;5;28mself\u001b[39m.beta / \u001b[32m2\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m d.detach().mean() > (\u001b[38;5;28mself\u001b[39m.d_targ * \u001b[32m1.5\u001b[39m):\n\u001b[32m    160\u001b[39m         \u001b[38;5;28mself\u001b[39m.beta = \u001b[38;5;28mself\u001b[39m.beta * \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1968\u001b[39m, in \u001b[36mModule.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   1963\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m   1964\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1969\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_from\u001b[39m(*dicts_or_sets) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1970\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dicts_or_sets:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi Ã¨ verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "#ppo.load_state_dict(torch.load(\"final.pt\"))\n",
    "ppo.train_model()\n",
    "ppo.save_parameters(\"final_adaptive.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a26235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep n 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_26421/1792810569.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep n 1 \n",
      "ep n 2 \n",
      "ep n 3 \n",
      "ep n 4 \n",
      "ep n 5 \n",
      "ep n 6 \n",
      "ep n 7 \n",
      "ep n 8 \n",
      "ep n 9 \n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "ppo = PPO(epochs=epochs, \n",
    "          training_iterations=training_iterations,\n",
    "          batch_size=batch_size,\n",
    "          trajectory_length=trajectory_length, \n",
    "          n_actors=n_actors,\n",
    "          env=env,\n",
    "          in_features=in_features,\n",
    "          out_features=out_features,\n",
    "          hidden_features=hidden_features,\n",
    "          device=device,\n",
    "          actor_learning_rate=actor_learning_rate,\n",
    "          critic_learning_rate=critic_learning_rate,\n",
    "          gamma=gamma,\n",
    "          lambda_=lambda_,\n",
    "          epsilon=epsilon,\n",
    "          beta = beta,\n",
    "          std=std,\n",
    "          mode = mode,\n",
    "        )\n",
    "\n",
    "ppo.load_state_dict(torch.load(\"final1.pt\"))\n",
    "\n",
    "env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1, render_mode=\"human\")\n",
    "#env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1)\n",
    "rewards = []\n",
    "for episode in range(10):\n",
    "    print(f\"ep n {episode}\", \"\\r\")\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    s, _ = env.reset()\n",
    "    while not done:\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        z, mu, std = ppo.actor(s)\n",
    "        s, reward, terminated, truncated, info = env.step(z.squeeze().cpu().detach().numpy())\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
