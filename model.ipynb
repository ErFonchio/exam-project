{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x12b7739b0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "\n",
    "# Profiler\n",
    "import wandb\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, critic_learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.critic_learning_rate)\n",
    "\n",
    "\n",
    "    def forward(self, in_):\n",
    "        if in_.dim() == 1:\n",
    "            in_ = in_.unsqueeze(0)\n",
    "        out = self.critic(in_)\n",
    "        return out\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, actor_learning_rate, std, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.std = std\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.out_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.actor_learning_rate)\n",
    "\n",
    "    def forward(self, in_):\n",
    "\n",
    "        if in_.dim() == 1:\n",
    "            in_ = in_.unsqueeze(0)\n",
    "        h = self.actor(in_)\n",
    "\n",
    "        epsilon = torch.randn(h.size(0), h.size(1)).to(self.device)\n",
    "        z = h + self.std * epsilon\n",
    "        return z, h, self.std\n",
    "    \n",
    "    def get_log_probability(self, z, mu, std):\n",
    "\n",
    "        dist = Normal(mu, std)\n",
    "        log_prob = dist.log_prob(z).sum(dim=-1)\n",
    "\n",
    "        return log_prob\n",
    "    \n",
    "class HalfCheetahBigLegEnv(HalfCheetahEnv):\n",
    "    def __init__(self, **kwargs):\n",
    "        xml_path = os.path.join(\n",
    "            os.path.dirname(__file__),\n",
    "            \"half_cheetah_bigleg.xml\"\n",
    "        )\n",
    "        super().__init__(xml_file=xml_path, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, epochs, training_iterations, batch_size, trajectory_length, \n",
    "                 n_actors, env1, env2, in_features, out_features, hidden_features, device, \n",
    "                 actor_learning_rate, critic_learning_rate, gamma, lambda_, epsilon, \n",
    "                 std, beta, d_targ, mode, n_nets, omega, omega12, run, toggle_log, alternating_step):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.training_iterations = training_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.n_actors = n_actors\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        self.omega = omega\n",
    "        self.omega12 = omega12\n",
    "        self.d_targ = d_targ\n",
    "        self.env1 = env1\n",
    "        self.env2 = env2\n",
    "        self.flag_env = 1\n",
    "        self.device = device\n",
    "        self.std = std\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        self.mode = mode\n",
    "        self.n_nets = n_nets\n",
    "        self.loss_coeff = beta * (omega ** torch.arange(0, self.n_nets)).to(device)\n",
    "        self.run = run\n",
    "        self.toggle_log = toggle_log\n",
    "        self.alternating_step = alternating_step\n",
    "\n",
    "        if n_nets > 1:\n",
    "            base_actor = Policy(\n",
    "                            in_features,\n",
    "                            out_features,\n",
    "                            hidden_features,\n",
    "                            actor_learning_rate,\n",
    "                            std,\n",
    "                            device\n",
    "                        ).to(self.device)\n",
    "\n",
    "            self.actor_list = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.n_nets):\n",
    "                actor = Policy(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    hidden_features,\n",
    "                    (self.omega**(-i)) * actor_learning_rate,\n",
    "                    std,\n",
    "                    device\n",
    "                ).to(self.device)\n",
    "                # the actors starts all with the same weights\n",
    "                actor.load_state_dict(base_actor.state_dict())\n",
    "                self.actor_list.append(actor)\n",
    "\n",
    "        else:\n",
    "            self.actor = Policy(in_features, out_features, hidden_features, actor_learning_rate, std, device)\n",
    "        \n",
    "        self.critic = ValueFunction(in_features, out_features, hidden_features, critic_learning_rate)\n",
    "\n",
    "    def switch_env(self):\n",
    "        print(\"Swapping task...\")\n",
    "        self.flag_env = (self.flag_env+1) % 2\n",
    "        if self.flag_env == 0:\n",
    "            return self.env1\n",
    "        return self.env2\n",
    "\n",
    "    def train_manager(self):\n",
    "        if self.mode == 'pc':\n",
    "            self.train_model_pc()\n",
    "        elif self.mode in ['clip', 'kl_fixed', 'kl_adaptive']:\n",
    "            self.train_model()\n",
    "        else:\n",
    "            print(\"[train_manager] method not allowed\")\n",
    "    \n",
    "    def train_model(self):\n",
    "\n",
    "        N = self.n_actors  #number of actors\n",
    "        T = self.trajectory_length # trajectory length\n",
    "        \n",
    "        for i in range(self.training_iterations):\n",
    "            dataset = []\n",
    "            print(f\"[train]: starting dataset creation at iteration n {i}\")\n",
    "\n",
    "            # decaying std\n",
    "            # self.std = self.std - 0.025*self.std\n",
    "            # self.run.log({\"std\": self.std})\n",
    "\n",
    "            if self.alternating_step > 0:\n",
    "                if (i % self.alternating_step) == 0:\n",
    "                    env = self.switch_env()\n",
    "                    self.std = 1\n",
    "            else:\n",
    "                env = self.env1\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                self.eval_model(env)\n",
    "                adv_list = []\n",
    "                cum_reward = 0\n",
    "                for _ in range(N): #for each actor\n",
    "\n",
    "                    # initialize first state\n",
    "                    s_prime, _ = env.reset()\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    trajectory = []\n",
    "\n",
    "                    for t in range(T):\n",
    "\n",
    "                        action, mu, std = self.actor(s_prime)\n",
    "                        log_policy = self.actor.get_log_probability(action, mu, std)\n",
    "\n",
    "                        s, reward, terminated, _, _ = env.step(action.squeeze(0).cpu().detach().numpy())\n",
    "                        s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                        reward = torch.tensor([[reward]], dtype=torch.float32).to(self.device)\n",
    "                        s_prime = s_prime.unsqueeze(0)\n",
    "                        trajectory.append([s_prime, action, reward, log_policy])\n",
    "                        s_prime = s\n",
    "                        cum_reward += reward\n",
    "\n",
    "                        if terminated:\n",
    "                            break\n",
    "\n",
    "                    dynamic_target = 0 if terminated else self.critic(s)\n",
    "                    for t in range(len(trajectory)-1, -1, -1): #I want the range from [T-1 to 0]\n",
    "                        \n",
    "                        dynamic_target = trajectory[t][2] + self.gamma*dynamic_target #taking the reward\n",
    "                        advantage = dynamic_target - self.critic(trajectory[t][0])\n",
    "                        trajectory[t] = tuple(trajectory[t] + [dynamic_target.unsqueeze(0), advantage.unsqueeze(0)])\n",
    "\n",
    "                        dataset.append(trajectory[t])\n",
    "                        adv_list.append(advantage)\n",
    "\n",
    "                adv_std, adv_mean = torch.std_mean(torch.tensor(adv_list))\n",
    "                print(f\"[training]: avg cum reward {cum_reward / N}\")\n",
    "                if self.toggle_log:\n",
    "                    self.run.log({\n",
    "                        \"avg cum_reward_training\": cum_reward / N,\n",
    "                        \"adv mean\": adv_mean,\n",
    "                        \"adv std\": adv_std,\n",
    "                        })\n",
    "            \n",
    "            print(f\"[training]: ending dataset creation with dataset size {len(dataset)}\")\n",
    "\n",
    "            self.actor.zero_grad()\n",
    "            self.critic.zero_grad()\n",
    "            # Starts the training process\n",
    "\n",
    "            for e in range(self.epochs):\n",
    "                \n",
    "                #print(f\"[train]: epoch n {e}\")\n",
    "                avg_loss_value = 0\n",
    "                avg_loss_ppo = 0\n",
    "                rd.shuffle(dataset) #shuffle in-place\n",
    "                \n",
    "                assert(self.batch_size <= len(dataset))\n",
    "\n",
    "                for mini_idx in range(0, len(dataset), self.batch_size):\n",
    "                    \n",
    "                    # form mini_batch\n",
    "                    mini_batch = dataset[mini_idx: mini_idx+self.batch_size]\n",
    "\n",
    "                    state_mini = torch.stack(list(map(lambda elem: elem[0].squeeze(), mini_batch)))\n",
    "                    action_mini = torch.stack(list(map(lambda elem: elem[1].squeeze(), mini_batch)))\n",
    "                    log_policy_mini = torch.stack(list(map(lambda elem: elem[3].squeeze(), mini_batch)))\n",
    "                    target_mini = torch.stack(list(map(lambda elem: elem[4].squeeze(), mini_batch)))\n",
    "                    advantage_mini = torch.stack(list(map(lambda elem: elem[5].squeeze(), mini_batch)))\n",
    "                    \n",
    "                    # Normalize advantage_mini\n",
    "                    advantage_mini = ((advantage_mini-adv_mean) / (adv_std+1e-8))\n",
    "\n",
    "                    _, mu_mini, std_mini = self.actor(state_mini) # std is a scalar!\n",
    "                    new_log_policy_mini = self.actor.get_log_probability(action_mini, mu_mini, std_mini)   \n",
    "\n",
    "                    new_value_mini = self.critic(state_mini)\n",
    "                    \n",
    "                    self.actor.optimizer.zero_grad()\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "                    \n",
    "                    if self.mode == 'clip':\n",
    "                        loss_ppo = self.loss_clip(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "                    elif (self.mode == 'kl_fixed') or (self.mode == 'kl_adaptive'):\n",
    "                        loss_ppo = self.loss_kl_standard(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "\n",
    "                    loss_value = self.loss_value(new_value_mini, target_mini)\n",
    "\n",
    "                    avg_loss_ppo += loss_ppo\n",
    "                    avg_loss_value += loss_value\n",
    "\n",
    "                    loss_ppo.backward()\n",
    "                    loss_value.backward()\n",
    "\n",
    "                    self.actor.optimizer.step()\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "                total_minibatch = math.floor(len(dataset) // self.batch_size)\n",
    "                print(f\"epoch {e} -> [avg actor loss]: {avg_loss_ppo / total_minibatch}  [critic loss]: {avg_loss_value / total_minibatch}\")\n",
    "                if self.toggle_log:\n",
    "                    self.run.log({\n",
    "                            \"avg_loss_ppo\": avg_loss_ppo / total_minibatch,\n",
    "                            \"avg_loss_value\": avg_loss_value / total_minibatch,\n",
    "                        })\n",
    "\n",
    "            #self.save_parameters(\"partial_models/model\"+str(i)+\".pt\")\n",
    "\n",
    "    def train_model_pc(self):\n",
    "\n",
    "        N = self.n_actors  #number of actors\n",
    "        T = self.trajectory_length # trajectory length\n",
    "        \n",
    "        for i in range(self.training_iterations):\n",
    "            dataset = []\n",
    "            print(f\"[train]: starting dataset creation at iteration n {i}\")\n",
    "\n",
    "            # decaying std\n",
    "            #self.std = self.std - 0.025*self.std\n",
    "            #self.run.log({\"std\": self.std})\n",
    "            \n",
    "            # alternating task\n",
    "            if self.alternating_step > 0:\n",
    "                if (i % self.alternating_step) == 0:\n",
    "                    env = self.switch_env()\n",
    "                    self.std = 1\n",
    "            else:\n",
    "                env = self.env1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                #self.eval_model(env)\n",
    "                adv_list = []\n",
    "                cum_reward = 0\n",
    "                for _ in range(N): #for each actor\n",
    "\n",
    "                    # initialize first state\n",
    "                    s_prime, _ = env.reset()\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    trajectory = []\n",
    "\n",
    "                    for t in range(T):\n",
    "                        \n",
    "                        actions_list_return = [self.actor_list[i](s_prime) for i in range(self.n_nets)] # [(action, mu, std), ..., ]\n",
    "                        log_policy_list = [self.actor_list[i].get_log_probability(actions_list_return[i][0],\n",
    "                                                                                actions_list_return[i][1],\n",
    "                                                                                actions_list_return[i][2]) for i in range(self.n_nets)] # [log_policy, ..., ]\n",
    "                        action_zero = actions_list_return[0][0]\n",
    "\n",
    "                        s, reward, terminated, _, _ = env.step(action_zero.squeeze(0).cpu().detach().numpy())\n",
    "                        s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                        reward = torch.tensor([[reward]], dtype=torch.float32).to(self.device)\n",
    "                        s_prime = s_prime.unsqueeze(0)\n",
    "                        trajectory.append([s_prime, action_zero, reward, log_policy_list])\n",
    "                        s_prime = s\n",
    "                        cum_reward += reward\n",
    "\n",
    "                        if terminated:\n",
    "                            break\n",
    "                        \n",
    "\n",
    "                    dynamic_target = 0 if terminated else self.critic(s)\n",
    "                    for t in range(len(trajectory)-1, -1, -1): #I want the range from [T-1 to 0]\n",
    "                        \n",
    "                        dynamic_target = trajectory[t][2] + self.gamma*dynamic_target #trajectory[t][2] = reward\n",
    "                        advantage = dynamic_target - self.critic(trajectory[t][0])\n",
    "                        trajectory[t] = tuple(trajectory[t] + [dynamic_target.unsqueeze(0), advantage.unsqueeze(0)])\n",
    "\n",
    "                        dataset.append(trajectory[t])\n",
    "                        adv_list.append(advantage)\n",
    "\n",
    "                adv_std, adv_mean = torch.std_mean(torch.tensor(adv_list))\n",
    "                print(f\"[training]: avg cum reward {cum_reward / N}\")\n",
    "                if self.toggle_log:\n",
    "                    self.run.log({\n",
    "                        \"avg cum_reward\": cum_reward / N,\n",
    "                        \"adv mean\": adv_mean,\n",
    "                        \"adv std\": adv_std,\n",
    "                        })\n",
    "\n",
    "            print(f\"[training]: ending dataset creation with dataset size {len(dataset)}\")\n",
    "\n",
    "            # Starts the training process\n",
    "            for e in range(self.epochs):\n",
    "                \n",
    "                #print(f\"[train]: epoch n {e}\")\n",
    "                avg_loss_value = 0\n",
    "                avg_loss_ppo = 0\n",
    "                rd.shuffle(dataset) #shuffle in-place\n",
    "                \n",
    "                assert(self.batch_size <= len(dataset))\n",
    "\n",
    "                for mini_idx in range(0, len(dataset), self.batch_size):\n",
    "                    \n",
    "                    # form mini_batch\n",
    "                    mini_batch = dataset[mini_idx: mini_idx+self.batch_size]\n",
    "                    # s, action, reward, log, target, advantage\n",
    "                    state_mini = torch.stack(list(map(lambda elem: elem[0].squeeze(), mini_batch)))\n",
    "                    action_mini = torch.stack(list(map(lambda elem: elem[1].squeeze(), mini_batch)))\n",
    "                    total_log_policy_list = torch.t(torch.stack(list(map(lambda elem: torch.cat(elem[3]), mini_batch)))) # size (nets, batch)\n",
    "                    target_mini = torch.stack(list(map(lambda elem: elem[4].squeeze(), mini_batch)))\n",
    "                    advantage_mini = torch.stack(list(map(lambda elem: elem[5].squeeze(), mini_batch)))\n",
    "                    \n",
    "                    # Normalize advantage_mini\n",
    "                    advantage_mini = ((advantage_mini-adv_mean) / (adv_std + 1e-8))\n",
    "                    new_actions = [actor(state_mini) for actor in self.actor_list]\n",
    "\n",
    "                    total_new_log_policy_list = torch.stack([self.actor_list[i].get_log_probability(action_mini, new_actions[i][1], new_actions[i][2]) for i in range(self.n_nets)]) # size (nets, batch)\n",
    "\n",
    "                    new_value_mini = self.critic(state_mini)\n",
    "                    \n",
    "                    [actor.zero_grad() for actor in self.actor_list]\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "                    \n",
    "                    # returns a loss for every net\n",
    "                    loss_ppo = self.loss_kl_pc(total_new_log_policy_list, total_log_policy_list, advantage_mini)\n",
    "                    loss_value = self.loss_value(new_value_mini, target_mini)\n",
    "\n",
    "                    avg_loss_ppo += loss_ppo\n",
    "                    avg_loss_value += loss_value\n",
    "\n",
    "                    loss_ppo.backward()\n",
    "                    loss_value.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    #[torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.8) for actor in self.actor_list]\n",
    "                    #torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.8)\n",
    "                    \n",
    "                    # optimization step\n",
    "                    [actor.optimizer.step() for actor in self.actor_list]\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "                total_minibatch = math.floor(len(dataset) // self.batch_size)\n",
    "                print(f\"epoch {e} -> [avg actor loss]: {avg_loss_ppo / total_minibatch}  [critic loss]: {avg_loss_value / total_minibatch}\")\n",
    "                if self.toggle_log:\n",
    "                    self.run.log({\n",
    "                            \"avg_loss_ppo\": avg_loss_ppo / total_minibatch,\n",
    "                            \"avg_loss_value\": avg_loss_value / total_minibatch,\n",
    "                        })\n",
    "\n",
    "            #self.save_parameters(\"partial_models/model\"+str(i)+\".pt\")\n",
    "\n",
    "    def loss_value(self, value, target):\n",
    "        #MSE\n",
    "        return torch.mean((value-target)**2)\n",
    "\n",
    "    def loss_clip(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "\n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini*advantage_mini\n",
    "        clip_ = torch.clip(prob_mini, 1-self.epsilon, 1+self.epsilon)*advantage_mini\n",
    "        return -torch.min(prob_adv, clip_).mean()\n",
    "    \n",
    "    def loss_kl_standard(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "        \n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini * advantage_mini\n",
    "        d = new_log_policy_mini - log_policy_mini\n",
    "\n",
    "        if self.mode == 'kl_adaptive':\n",
    "            if d.detach().mean() < (self.d_targ / 1.5):\n",
    "                self.beta = self.beta / 2.3\n",
    "            elif d.detach().mean() > (self.d_targ * 1.5):\n",
    "                self.beta = self.beta * 1.9\n",
    "\n",
    "        return -(prob_adv - self.beta*d).mean()\n",
    "    \n",
    "    def loss_kl_pc(self, stack_new, stack_old, advantage_mini):\n",
    "        \n",
    "        # stack new and stack old have shape (n_net, batch_size)\n",
    "        # We compute the policy gradient based on first net prob and adv\n",
    "        new_log_policy_mini = stack_new[0, :]\n",
    "        log_policy_mini = stack_old[0, :]\n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        L_pg = prob_mini * advantage_mini\n",
    "\n",
    "        kl_stack = stack_new - stack_old\n",
    "        L_ppo = torch.sum(self.loss_coeff * torch.t(kl_stack), dim=1)\n",
    "\n",
    "        L_casc_init = self.omega12 * (torch.t(stack_new)[:, 0] - torch.t(stack_old)[:, 1])\n",
    "        kl_sub_previous = torch.t(stack_new)[:, 1:] - torch.t(stack_old)[:, 0:stack_old.shape[0]-1]\n",
    "        # I'm appending to the matrix a row which is equal to the last row. At the end i will have a matrix with\n",
    "        # We also don't need the first two columns of old\n",
    "        if self.n_nets > 2:\n",
    "            kl_sub_successive_second = torch.t(torch.cat((stack_old, stack_new[stack_new.shape[0]-1, :].unsqueeze(0)), 0))[:, 2:]\n",
    "        else:\n",
    "            kl_sub_successive_second = 0\n",
    "        kl_sub_successive = torch.t(stack_new)[:, 1:] - kl_sub_successive_second\n",
    "        L_casc = L_casc_init + torch.sum(self.omega*kl_sub_previous + kl_sub_successive, dim=1) # summing on net dimension\n",
    "        \n",
    "        if self.toggle_log:\n",
    "            self.run.log({\n",
    "                \"L_pg\": L_pg.mean(),\n",
    "                \"L_ppo\": L_ppo.mean(),\n",
    "                \"L_casc\": L_casc.mean()\n",
    "            })\n",
    "\n",
    "        return -(L_pg - L_ppo - L_casc).mean()\n",
    "    \n",
    "\n",
    "    def extract_states_prime(self, trajectory):\n",
    "        return list(map(lambda x: x[0], trajectory))\n",
    "    \n",
    "    def save_parameters(self, path):\n",
    "        torch.save({\n",
    "            \"model_state_dict\": self.state_dict()\n",
    "        }, path + \"/model_state_dict.pt\")\n",
    "\n",
    "        torch.save({\n",
    "            \"beta\": self.beta,\n",
    "            \"omega\": self.omega,\n",
    "            \"omega12\": self.omega12,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"std\": self.std,\n",
    "            \"d_targ\": self.d_targ,\n",
    "            \"n_nets\": self.n_nets,\n",
    "            \"actor_learning_rate\": self.actor_learning_rate,\n",
    "            \"critic_learning_rate\": self.critic_learning_rate,\n",
    "        }, path + \"/config.pt\")\n",
    "\n",
    "    def load_parameters(self, path):\n",
    "\n",
    "        # loading parameters\n",
    "        checkpoint = torch.load(path + \"/model_state_dict.pt\", map_location=self.device)\n",
    "        self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        # loading configuration\n",
    "        config = torch.load(path + \"/config.pt\")\n",
    "        self.beta = config[\"beta\"]\n",
    "        self.omega = config[\"omega\"]\n",
    "        self.omega12 = config[\"omega12\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.epsilon = config[\"epsilon\"]\n",
    "        self.std = config[\"std\"]\n",
    "        self.d_targ = config[\"d_targ\"]\n",
    "        self.n_nets = config[\"n_nets\"]\n",
    "        self.actor_learning_rate = config[\"actor_learning_rate\"]\n",
    "        self.critic_learning_rate = config[\"critic_learning_rate\"]\n",
    "\n",
    "    def eval_model(self, env):\n",
    "\n",
    "        total_run = 5\n",
    "        cum_reward = 0\n",
    "        for _ in range(total_run):\n",
    "            s_prime, _ = env.reset()\n",
    "            s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "            for _ in range(self.trajectory_length):\n",
    "\n",
    "                if self.mode == 'pc':\n",
    "                    _, mu, _ = self.actor_list[0](s_prime)\n",
    "                else:\n",
    "                    _, mu, _ = self.actor(s_prime)\n",
    "\n",
    "                s, reward, terminated, _, _ = env.step(mu.squeeze(0).cpu().detach().numpy())\n",
    "                s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                s_prime = s_prime.unsqueeze(0)\n",
    "                s_prime = s\n",
    "                cum_reward += reward\n",
    "                if terminated:\n",
    "                    break\n",
    "\n",
    "        print(\"avg cum_reward_eval: \", cum_reward / total_run)\n",
    "        if (cum_reward / total_run) > 800:\n",
    "            self.save_parameters(path=\"partial_models\"+str(rd.randint(0, 1000)))\n",
    "            \n",
    "        if self.toggle_log:\n",
    "            self.run.log({\n",
    "                \"avg cum_reward_eval\": cum_reward / total_run\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "566d52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_space = \"HalfCheetah-v5\"\n",
    "env1 = gym.make(env_space, ctrl_cost_weight=0.1)\n",
    "env2 = HalfCheetahEnv(xml_file=\"half_cheetah_bigleg.xml\", ctrl_cost_weight=0.1)\n",
    "\n",
    "epochs = 10\n",
    "training_iterations = 20\n",
    "batch_size = 64\n",
    "trajectory_length = 800\n",
    "n_actors = 10\n",
    "in_features = env1.observation_space.shape[0]\n",
    "out_features = env1.action_space.shape[0]\n",
    "hidden_features = 64\n",
    "actor_learning_rate = 5e-5\n",
    "critic_learning_rate = 5e-5\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "epsilon = 0.2\n",
    "beta = 0.5\n",
    "omega = 4\n",
    "omega12 = 1\n",
    "d_targ = 0.01\n",
    "std = 0.2\n",
    "n_nets = 7\n",
    "toggle_log = False\n",
    "alternating_step = 0\n",
    "\n",
    "device = \"mps\"\n",
    "mode = \"clip\"\n",
    "\n",
    "if mode == \"pc\":\n",
    "    assert(n_nets > 1)\n",
    "modes = [\"kl_fixed\", \"kl_adaptive\", \"clip\", \"pc\"]\n",
    "assert(mode in modes)\n",
    "\n",
    "run = None\n",
    "if toggle_log:\n",
    "    run = wandb.init(\n",
    "        entity=\"alecoccia-sapienza-universit-di-roma\",\n",
    "        project=\"RL\",\n",
    "        config = {\n",
    "            \"env_name\": env_space,\n",
    "            \"epochs\": epochs,\n",
    "            \"training_iterations\": training_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"trajectory_length\": trajectory_length,\n",
    "            \"n_actors\": n_actors,\n",
    "            \"in_features\": in_features,\n",
    "            \"out_features\": out_features,\n",
    "            \"hidden_features\": hidden_features,\n",
    "            \"actor_learning_rate\": actor_learning_rate,\n",
    "            \"critic_learning_rate\": critic_learning_rate,\n",
    "            \"gamma\": gamma,\n",
    "            \"lambda_\": lambda_,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"beta\": beta,\n",
    "            \"omega\": omega,\n",
    "            \"omega12\": omega12,\n",
    "            \"d_targ\": d_targ,\n",
    "            \"std\": std,\n",
    "            \"n_nets\": n_nets,\n",
    "            \"device\": device,\n",
    "            \"mode\": mode,\n",
    "            \"alternating_step\": alternating_step,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f3dabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo = PPO(epochs=epochs,\n",
    "#           training_iterations=training_iterations,\n",
    "#           batch_size=batch_size,\n",
    "#           trajectory_length=trajectory_length, \n",
    "#           n_actors=n_actors,\n",
    "#           env1=env1,\n",
    "#           env2=env2,\n",
    "#           in_features=in_features,\n",
    "#           out_features=out_features,\n",
    "#           hidden_features=hidden_features,\n",
    "#           device=device,\n",
    "#           actor_learning_rate=actor_learning_rate,\n",
    "#           critic_learning_rate=critic_learning_rate,\n",
    "#           gamma=gamma,\n",
    "#           lambda_=lambda_,\n",
    "#           epsilon=epsilon,\n",
    "#           beta = beta,\n",
    "#           d_targ=d_targ,\n",
    "#           std=std,\n",
    "#           mode=mode,\n",
    "#           n_nets=n_nets,\n",
    "#           omega=omega,\n",
    "#           omega12=omega12,\n",
    "#           run=run,\n",
    "#           toggle_log=toggle_log,\n",
    "#           alternating_step=alternating_step\n",
    "#         )\n",
    "# ppo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "329654a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ppo.load_state_dict(torch.load(\"clip2_half.pt\"))\n",
    "# if toggle_log:\n",
    "#     name = \"pc_single2\"\n",
    "#     ppo.load_parameters(\"final_models/\" + \"pc_1_alternating_correct\")\n",
    "\n",
    "#     ppo.train_manager()\n",
    "    \n",
    "#     os.makedirs(\"final_models/\" + name, exist_ok=True)\n",
    "#     ppo.save_parameters(\"final_models/\" + name)\n",
    "\n",
    "#     run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69a26235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_64810/112192301.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380.6389626274854\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "ppo = PPO(epochs=epochs,\n",
    "          training_iterations=training_iterations,\n",
    "          batch_size=batch_size,\n",
    "          trajectory_length=trajectory_length, \n",
    "          n_actors=n_actors,\n",
    "          env1=env1,\n",
    "          env2=env2,\n",
    "          in_features=in_features,\n",
    "          out_features=out_features,\n",
    "          hidden_features=hidden_features,\n",
    "          device=device,\n",
    "          actor_learning_rate=actor_learning_rate,\n",
    "          critic_learning_rate=critic_learning_rate,\n",
    "          gamma=gamma,\n",
    "          lambda_=lambda_,\n",
    "          epsilon=epsilon,\n",
    "          beta = beta,\n",
    "          d_targ=d_targ,\n",
    "          std=std,\n",
    "          mode=mode,\n",
    "          n_nets=n_nets,\n",
    "          omega=omega,\n",
    "          omega12=omega12,\n",
    "          run=run,\n",
    "          toggle_log=toggle_log,\n",
    "          alternating_step=alternating_step,\n",
    ")\n",
    "\n",
    "#ppo.load_state_dict(torch.load(\"final_models/trial.pt\"))\n",
    "ppo.load_parameters(\"partial_models1051\")\n",
    "#ppo.load_state_dict(torch.load(\"final_models/clip_single/model_state_dict.pt\"))\n",
    "tmp_env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1, render_mode=\"rgb_array\")\n",
    "#tmp_env = HalfCheetahEnv(xml_file=\"half_cheetah_bigleg.xml\", ctrl_cost_weight=0.1, render_mode=\"rgb_array\")\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env=tmp_env, video_folder=\"video/pc_alternating-video\", name_prefix=\"halfcheetah-v5\", episode_trigger=lambda x: x % 2 == 0)\n",
    "\n",
    "env.start_recording(\"pc\")\n",
    "total_reward = 0\n",
    "done = False\n",
    "s, _ = env.reset()\n",
    "for i in range(1000):\n",
    "    s = torch.tensor(s, dtype=torch.float32)\n",
    "    z, mu, std = ppo.actor_list[0](s)\n",
    "    #z, mu, std = ppo.actor(s)\n",
    "    # we use mu\n",
    "    s, reward, terminated, truncated, info = env.step(mu.squeeze(0).cpu().detach().numpy())\n",
    "    s = torch.tensor(s, dtype=torch.float32)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "print(total_reward)\n",
    "\n",
    "env.stop_recording()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d4ba14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 0.5, 'omega': 2, 'omega12': 1, 'epsilon': 0.2, 'gamma': 0.99, 'std': 0.25, 'd_targ': 0.01, 'n_nets': 1, 'actor_learning_rate': 0.0005, 'critic_learning_rate': 0.0005}\n"
     ]
    }
   ],
   "source": [
    "config = torch.load(\"final_models/clip_single/config.pt\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d08de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_list = os.listdir()\n",
    "# for elem in folder_list:\n",
    "#     if (\"partial_models\" in elem) and len(elem) > 14:\n",
    "#         # if \"025\" in elem:\n",
    "#         #     config = torch.load(elem + \"/config.pt\")\n",
    "#         #     print(\"elem: \", elem)\n",
    "#         #     print(\"config: \", config)\n",
    "#         if elem[-1] == \"1\":\n",
    "#             config = torch.load(elem + \"/config.pt\")\n",
    "#             print(\"elem: \", elem)\n",
    "#             print(\"config: \", config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85a418e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BEST RUNS\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "# ppo = PPO(epochs=epochs,\n",
    "#           training_iterations=training_iterations,\n",
    "#           batch_size=batch_size,\n",
    "#           trajectory_length=trajectory_length, \n",
    "#           n_actors=n_actors,\n",
    "#           env1=env1,\n",
    "#           env2=env2,\n",
    "#           in_features=in_features,\n",
    "#           out_features=out_features,\n",
    "#           hidden_features=hidden_features,\n",
    "#           device=device,\n",
    "#           actor_learning_rate=actor_learning_rate,\n",
    "#           critic_learning_rate=critic_learning_rate,\n",
    "#           gamma=gamma,\n",
    "#           lambda_=lambda_,\n",
    "#           epsilon=epsilon,\n",
    "#           beta = beta,\n",
    "#           d_targ=d_targ,\n",
    "#           std=std,\n",
    "#           mode=mode,\n",
    "#           n_nets=n_nets,\n",
    "#           omega=omega,\n",
    "#           omega12=omega12,\n",
    "#           run=run,\n",
    "#           toggle_log=toggle_log,\n",
    "#           alternating_step=alternating_step,\n",
    "# )\n",
    "\n",
    "# #ppo.load_state_dict(torch.load(\"final_models/clip_single/model_state_dict.pt\"))\n",
    "# ppo.load_parameters(\"final_models/clip_alternating_correct\")\n",
    "\n",
    "# #env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1)\n",
    "# env = HalfCheetahEnv(xml_file=\"half_cheetah_bigleg.xml\", ctrl_cost_weight=0.1)\n",
    "\n",
    "# total_reward = 0\n",
    "# for episode in range(10):\n",
    "#     print(f\"ep n {episode}\", \"\\r\")\n",
    "#     s, _ = env.reset()\n",
    "#     for i in range(1000):\n",
    "#         s = torch.tensor(s, dtype=torch.float32)\n",
    "#         #z, mu, std = ppo.actor_list[0](s)\n",
    "#         z, mu, std = ppo.actor(s)\n",
    "#         s, reward, terminated, truncated, info = env.step(mu.squeeze(0).cpu().detach().numpy())\n",
    "#         s = torch.tensor(s, dtype=torch.float32)\n",
    "#         total_reward += reward\n",
    "\n",
    "# print(total_reward / 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
