{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02baae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x31b4d0e90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "\n",
    "# Profiler\n",
    "import cProfile\n",
    "import re\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfb132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, critic_learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.critic_learning_rate)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.critic(input)\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, actor_learning_rate, std, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.std = std\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_features, self.out_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.actor_learning_rate)\n",
    "\n",
    "    def forward(self, in_):\n",
    "\n",
    "        if in_.dim() == 1:\n",
    "            in_ = in_.unsqueeze(0)\n",
    "        h = self.actor(in_)\n",
    "\n",
    "        epsilon = torch.randn(h.size(0), h.size(1)).to(self.device)\n",
    "        z = h + self.std * epsilon\n",
    "        return z, h, self.std\n",
    "    \n",
    "    def get_log_probability(self, z, mu, std):\n",
    "\n",
    "        coeff =  1 / (std*math.sqrt(2*math.pi))\n",
    "        normal_dist = coeff * torch.exp(-0.5 * (((z - mu) / std) ** 2) )\n",
    "        assert(normal_dist.all() >= 0)\n",
    "\n",
    "        return torch.log(normal_dist).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, epochs, training_iterations, batch_size, trajectory_length, n_actors, env, in_features, out_features, hidden_features, device, actor_learning_rate, critic_learning_rate, gamma, lambda_, epsilon, std, beta, d_targ, mode, n_nets, omega, omega12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.training_iterations = training_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.n_actors = n_actors\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        self.omega = omega\n",
    "        self.omega12 = omega12\n",
    "        self.d_targ = d_targ\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.std = std\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.mode = mode\n",
    "        self.n_nets = n_nets\n",
    "        self.loss_coeff = beta * (omega ** torch.arange(0, self.n_nets)).to(device)\n",
    "\n",
    "        if n_nets > 1:\n",
    "            self.actor_list = [Policy(in_features, \n",
    "                                out_features, \n",
    "                                hidden_features, \n",
    "                                (self.omega**(-i))*actor_learning_rate,\n",
    "                                std,\n",
    "                                device\n",
    "                                ).to(self.device) for i in range(self.n_nets)]\n",
    "\n",
    "        else:\n",
    "            self.actor = Policy(in_features, out_features, hidden_features, actor_learning_rate, std, device)\n",
    "        \n",
    "        self.critic = ValueFunction(in_features, out_features, hidden_features, critic_learning_rate)\n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "\n",
    "        N = self.n_actors  #number of actors\n",
    "        T = self.trajectory_length # trajectory length\n",
    "        \n",
    "        for i in range(self.training_iterations):\n",
    "            dataset = []\n",
    "            print(f\"[train]: starting dataset creation at iteration n {i}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                adv_list = []\n",
    "                cum_reward = 0\n",
    "                for _ in range(N): #for each actor\n",
    "\n",
    "                    # initialize first state\n",
    "                    s_prime, _ = self.env.reset()\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    trajectory = []\n",
    "                    done = False\n",
    "\n",
    "                    for t in range(T):\n",
    "\n",
    "                        action, mu, std = self.actor(s_prime)\n",
    "                        log_policy = self.actor.get_log_probability(action, mu, std)\n",
    "\n",
    "                        s, reward, terminated, truncated, _ = self.env.step(action.squeeze(0).cpu().detach().numpy())\n",
    "                        s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                        reward = torch.tensor([[reward]], dtype=torch.float32).to(self.device)\n",
    "                        s_prime = s_prime.unsqueeze(0)\n",
    "                        trajectory.append([s_prime, action, reward, log_policy])\n",
    "                        s_prime = s\n",
    "                        cum_reward += reward\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "\n",
    "                    dynamic_target = 0 if done else self.critic(s)\n",
    "                    for t in range(len(trajectory)-1, -1, -1): #I want the range from [T-1 to 0]\n",
    "                        \n",
    "                        dynamic_target = dynamic_target*self.gamma + trajectory[t][2] #taking the reward\n",
    "                        advantage = dynamic_target - self.critic(trajectory[t][0])\n",
    "                        trajectory[t] = tuple(trajectory[t] + [dynamic_target.unsqueeze(0), advantage.unsqueeze(0)])\n",
    "\n",
    "                        dataset.append(trajectory[t])\n",
    "                        adv_list.append(advantage)\n",
    "\n",
    "                adv_std, adv_mean = torch.std_mean(torch.tensor(adv_list))\n",
    "                print(f\"[training]: cum reward {cum_reward}\")\n",
    "            \n",
    "            print(f\"[training]: ending dataset creation with dataset size {len(dataset)}\")\n",
    "\n",
    "            self.actor.zero_grad()\n",
    "            self.critic.zero_grad()\n",
    "            # Starts the training process\n",
    "            for e in range(self.epochs):\n",
    "                \n",
    "                print(f\"[train]: epoch n {e}\")\n",
    "                avg_loss_value = 0\n",
    "                avg_loss_ppo = 0\n",
    "                rd.shuffle(dataset) #shuffle in-place\n",
    "                \n",
    "                assert(self.batch_size <= len(dataset))\n",
    "\n",
    "                for mini_idx in range(0, len(dataset), self.batch_size):\n",
    "                    \n",
    "                    # form mini_batch\n",
    "                    mini_batch = dataset[mini_idx: mini_idx+self.batch_size]\n",
    "\n",
    "                    state_mini = torch.stack(list(map(lambda elem: elem[0].squeeze(), mini_batch)))\n",
    "                    action_mini = torch.stack(list(map(lambda elem: elem[1].squeeze(), mini_batch)))\n",
    "                    log_policy_mini = torch.stack(list(map(lambda elem: elem[3].squeeze(), mini_batch)))\n",
    "                    advantage_mini = torch.stack(list(map(lambda elem: elem[4].squeeze(), mini_batch)))\n",
    "                    target_mini = torch.stack(list(map(lambda elem: elem[5].squeeze(), mini_batch)))\n",
    "                    \n",
    "                    # Normalize advantage_mini\n",
    "                    advantage_mini = ((advantage_mini-adv_mean) / (adv_std+0.00001))\n",
    "\n",
    "                    _, mu_mini, std_mini = self.actor(state_mini) # std is a scalar!\n",
    "                    new_log_policy_mini = self.actor.get_log_probability(action_mini, mu_mini, std_mini)   \n",
    "\n",
    "                    new_value_mini = self.critic(state_mini)\n",
    "                    \n",
    "                    self.actor.optimizer.zero_grad()\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "                    \n",
    "                    if self.mode == 'clip':\n",
    "                        loss_ppo = self.loss_clip(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "                    elif (self.mode == 'kl_fixed') or (self.mode == 'kl_adaptive'):\n",
    "                        loss_ppo = self.loss_kl(new_log_policy_mini, log_policy_mini, advantage_mini)\n",
    "\n",
    "                    loss_value = self.loss_value(new_value_mini, target_mini)\n",
    "\n",
    "                    avg_loss_ppo += loss_ppo\n",
    "                    avg_loss_value += avg_loss_value\n",
    "\n",
    "                    loss_ppo.backward()\n",
    "                    loss_value.backward()\n",
    "                    \n",
    "                    self.actor.optimizer.step()\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "\n",
    "                total_minibatch = math.floor(len(dataset) // self.batch_size)\n",
    "                print(f\"[avg actor loss]: {avg_loss_ppo / total_minibatch} \\t[critic loss]: {loss_value / total_minibatch}\")\n",
    "\n",
    "\n",
    "            self.save_parameters(\"model\"+str(i)+\".pt\")\n",
    "\n",
    "\n",
    "    def train_model_pc(self):\n",
    "\n",
    "        N = self.n_actors  #number of actors\n",
    "        T = self.trajectory_length # trajectory length\n",
    "        \n",
    "        for i in range(self.training_iterations):\n",
    "            dataset = []\n",
    "            print(f\"[train]: starting dataset creation at iteration n {i}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                adv_list = []\n",
    "                cum_reward = 0\n",
    "                for _ in range(N): #for each actor\n",
    "\n",
    "                    # initialize first state\n",
    "                    s_prime, _ = self.env.reset()\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    trajectory = []\n",
    "                    done = False\n",
    "\n",
    "                    for t in range(T):\n",
    "                        \n",
    "                        actions_list_return = [self.actor_list[i](s_prime) for i in range(self.n_nets)] # [(action, mu, std), ..., ]\n",
    "                        log_policy_list = [self.actor_list[i].get_log_probability(actions_list_return[i][0],\n",
    "                                                                                actions_list_return[i][1],\n",
    "                                                                                actions_list_return[i][2]) for i in range(self.n_nets)] # [log_policy, ..., ]\n",
    "                        action_zero = actions_list_return[0][0]\n",
    "\n",
    "                        s, reward, terminated, truncated, _ = self.env.step(action_zero.squeeze(0).cpu().detach().numpy())\n",
    "                        s = torch.tensor(s, dtype=torch.float32).to(self.device)\n",
    "                        reward = torch.tensor([[reward]], dtype=torch.float32).to(self.device)\n",
    "                        s_prime = s_prime.unsqueeze(0)\n",
    "                        trajectory.append([s_prime, action_zero, reward, log_policy_list])\n",
    "                        s_prime = s\n",
    "                        cum_reward += reward\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                    dynamic_target = 0 if done else self.critic(s)\n",
    "                    for t in range(len(trajectory)-1, -1, -1): #I want the range from [T-1 to 0]\n",
    "                        \n",
    "                        dynamic_target = dynamic_target*self.gamma + trajectory[t][2] #taking the reward\n",
    "                        advantage = dynamic_target - self.critic(trajectory[t][0])\n",
    "                        trajectory[t] = tuple(trajectory[t] + [dynamic_target.unsqueeze(0), advantage.unsqueeze(0)])\n",
    "\n",
    "                        dataset.append(trajectory[t])\n",
    "                        adv_list.append(advantage)\n",
    "\n",
    "                adv_std, adv_mean = torch.std_mean(torch.tensor(adv_list))\n",
    "                print(f\"[training]: cum reward {cum_reward}\")\n",
    "            \n",
    "            print(f\"[training]: ending dataset creation with dataset size {len(dataset)}\")\n",
    "\n",
    "            [actor.zero_grad() for actor in self.actor_list]\n",
    "            self.critic.zero_grad()\n",
    "            # Starts the training process\n",
    "            for e in range(self.epochs):\n",
    "                \n",
    "                print(f\"[train]: epoch n {e}\")\n",
    "                avg_loss_value = 0\n",
    "                avg_loss_ppo = 0\n",
    "                rd.shuffle(dataset) #shuffle in-place\n",
    "                \n",
    "                assert(self.batch_size <= len(dataset))\n",
    "\n",
    "                for mini_idx in range(0, len(dataset), self.batch_size):\n",
    "                    \n",
    "                    # form mini_batch\n",
    "                    mini_batch = dataset[mini_idx: mini_idx+self.batch_size]\n",
    "\n",
    "                    state_mini = torch.stack(list(map(lambda elem: elem[0].squeeze(), mini_batch)))\n",
    "                    action_mini = torch.stack(list(map(lambda elem: elem[1].squeeze(), mini_batch)))\n",
    "                    total_log_policy_list = torch.t(torch.stack(list(map(lambda elem: torch.cat(elem[3]), mini_batch)))) # size (nets, batch)\n",
    "                    advantage_mini = torch.stack(list(map(lambda elem: elem[4].squeeze(), mini_batch)))\n",
    "                    target_mini = torch.stack(list(map(lambda elem: elem[5].squeeze(), mini_batch)))\n",
    "                    \n",
    "                    # Normalize advantage_mini\n",
    "                    advantage_mini = ((advantage_mini-adv_mean) / (adv_std+0.00001))\n",
    "                    new_actions = [actor(state_mini) for actor in self.actor_list]\n",
    "\n",
    "                    total_new_log_policy_list = torch.stack([self.actor_list[i].get_log_probability(action_mini, new_actions[i][1], new_actions[i][2]) for i in range(self.n_nets)]) # size (nets, batch)\n",
    "\n",
    "                    new_value_mini = self.critic(state_mini)\n",
    "                    \n",
    "                    [actor.zero_grad() for actor in self.actor_list]\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "                    \n",
    "                    # returns a loss for every net\n",
    "                    loss_ppo = self.loss_kl(total_new_log_policy_list, total_log_policy_list, advantage_mini)\n",
    "                    loss_value = self.loss_value(new_value_mini, target_mini)\n",
    "\n",
    "                    avg_loss_ppo += loss_ppo\n",
    "                    avg_loss_value += loss_value\n",
    "\n",
    "                    loss_ppo.backward()\n",
    "                    loss_value.backward()\n",
    "                    \n",
    "                    [actor.optimizer.step() for actor in self.actor_list]\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "\n",
    "                total_minibatch = math.floor(len(dataset) // self.batch_size)\n",
    "                print(f\"[avg actor loss]: {avg_loss_ppo / total_minibatch} \\t[critic loss]: {avg_loss_value / total_minibatch}\")\n",
    "\n",
    "\n",
    "            self.save_parameters(\"partial_models/model\"+str(i)+\".pt\")\n",
    "\n",
    "                \n",
    "\n",
    "    def loss_value(self, value, target):\n",
    "        #MSE\n",
    "        return torch.mean((value-target)**2)\n",
    "\n",
    "    def loss_clip(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "\n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini*advantage_mini\n",
    "        clip_ = torch.clip(prob_mini, 1-self.epsilon, 1+self.epsilon)*advantage_mini\n",
    "        return -torch.min(prob_adv, clip_).mean()\n",
    "    \n",
    "    def loss_kl1(self, new_log_policy_mini, log_policy_mini, advantage_mini):\n",
    "        \n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "        prob_adv = prob_mini * advantage_mini\n",
    "        d = log_policy_mini - new_log_policy_mini\n",
    "\n",
    "        if self.mode == 'kl_adaptive':\n",
    "            if d.detach().mean() < (self.d_targ / 1.5):\n",
    "                self.beta = self.beta / 2\n",
    "            elif d.detach().mean() > (self.d_targ * 1.5):\n",
    "                self.beta = self.beta * 2\n",
    "\n",
    "        return -(prob_adv - self.beta*d).mean()\n",
    "    \n",
    "    def loss_kl(self, stack_new, stack_old, advantage_mini):\n",
    "        \n",
    "        # print(\"total_new_log_policy_list\", stack_new)\n",
    "        # print(\"total_log_policy_list\", stack_old)\n",
    "        # stack new and stack old have shape (n_net, batch_size)\n",
    "        # We compute the policy gradient based on first net prob and adv\n",
    "        new_log_policy_mini = stack_new[0, :]\n",
    "        log_policy_mini = stack_old[0, :]\n",
    "        prob_mini = torch.exp(new_log_policy_mini - log_policy_mini)\n",
    "\n",
    "        L_pg = prob_mini * advantage_mini\n",
    "\n",
    "        kl_stack = stack_new - stack_old\n",
    "        L_ppo = torch.sum(self.loss_coeff * torch.t(kl_stack), dim=1)\n",
    "\n",
    "        # print(\"stack_new\", torch.t(stack_new))\n",
    "        # print(\"stack_old\", torch.t(stack_old))\n",
    "        # print(\"stack_new_sliced\", torch.t(stack_new)[:, 1:])\n",
    "        # print(\"stack_old_sliced\", torch.t(stack_old)[:, 0:stack_old.shape[0]-1])\n",
    "\n",
    "\n",
    "        L_casc_init = self.omega12 * (torch.t(stack_new)[:, 1] - torch.t(stack_old)[:, 2])\n",
    "        kl_sub_previous = torch.t(stack_new)[:, 1:] - torch.t(stack_old)[:, 0:stack_old.shape[0]-1]\n",
    "        # I'm appending to the matrix a row which is equal to the last row. At the end i will have a matrix with\n",
    "        # We also don't need the first two columns of old\n",
    "        kl_sub_successive = torch.t(stack_new)[:, 1:] - torch.t(torch.cat((stack_old, stack_new[stack_new.shape[0]-1, :].unsqueeze(0)), 0))[:, 2:]\n",
    "        L_casc = L_casc_init + torch.sum(self.omega*kl_sub_previous + kl_sub_successive, dim=1) # summing on net dimension\n",
    "\n",
    "        return -(L_pg - L_ppo - L_casc).mean()\n",
    "        \n",
    "\n",
    "    def extract_states_prime(self, trajectory):\n",
    "        return list(map(lambda x: x[0], trajectory))\n",
    "    \n",
    "    def save_parameters(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "566d52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1)\n",
    "\n",
    "epochs = 10\n",
    "training_iterations = 20\n",
    "batch_size = 64\n",
    "trajectory_length = 500\n",
    "n_actors = 10\n",
    "in_features = env.observation_space.shape[0]\n",
    "out_features = env.action_space.shape[0]\n",
    "hidden_features = 64\n",
    "actor_learning_rate = 5e-4\n",
    "critic_learning_rate = 5e-4\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "epsilon = 0.2\n",
    "beta = 0.5\n",
    "omega = 1\n",
    "omega12 = 1\n",
    "d_targ = 0.01\n",
    "std = 0.5\n",
    "n_nets = 7\n",
    "\n",
    "device = \"mps\"\n",
    "mode = \"pc\"\n",
    "if mode == \"pc\":\n",
    "    assert(n_nets > 1)\n",
    "modes = [\"kl_fixed\", \"kl_adaptive\", \"clip\", \"pc\"]\n",
    "assert(mode in modes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f3dabe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (critic): ValueFunction(\n",
       "    (critic): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo = PPO(epochs=epochs, \n",
    "          training_iterations=training_iterations,\n",
    "          batch_size=batch_size,\n",
    "          trajectory_length=trajectory_length, \n",
    "          n_actors=n_actors,\n",
    "          env=env,\n",
    "          in_features=in_features,\n",
    "          out_features=out_features,\n",
    "          hidden_features=hidden_features,\n",
    "          device=device,\n",
    "          actor_learning_rate=actor_learning_rate,\n",
    "          critic_learning_rate=critic_learning_rate,\n",
    "          gamma=gamma,\n",
    "          lambda_=lambda_,\n",
    "          epsilon=epsilon,\n",
    "          beta = beta,\n",
    "          d_targ=d_targ,\n",
    "          std=std,\n",
    "          mode=mode,\n",
    "          n_nets=n_nets,\n",
    "          omega=omega,\n",
    "          omega12=omega12,\n",
    "        )\n",
    "ppo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329654a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: starting dataset creation at iteration n 0\n",
      "[training]: cum reward tensor([[-1631.0850]], device='mps:0')\n",
      "[training]: ending dataset creation with dataset size 5000\n",
      "[train]: epoch n 0\n",
      "loss pg:  tensor(-0.0004, device='mps:0')\n",
      "loss ppo:  tensor(-6.8986, device='mps:0')\n",
      "loss casc:  tensor(-26.8044, device='mps:0')\n",
      "loss pg:  tensor(-0.1911, device='mps:0')\n",
      "loss ppo:  tensor(-8.7455, device='mps:0')\n",
      "loss casc:  tensor(-32.6794, device='mps:0')\n",
      "loss pg:  tensor(0.0141, device='mps:0')\n",
      "loss ppo:  tensor(-8.7591, device='mps:0')\n",
      "loss casc:  tensor(-33.8067, device='mps:0')\n",
      "loss pg:  tensor(0.0104, device='mps:0')\n",
      "loss ppo:  tensor(-7.8589, device='mps:0')\n",
      "loss casc:  tensor(-30.1538, device='mps:0')\n",
      "loss pg:  tensor(0.1816, device='mps:0')\n",
      "loss ppo:  tensor(-10.1987, device='mps:0')\n",
      "loss casc:  tensor(-40.2746, device='mps:0')\n",
      "loss pg:  tensor(-0.1065, device='mps:0')\n",
      "loss ppo:  tensor(-11.8191, device='mps:0')\n",
      "loss casc:  tensor(-45.0157, device='mps:0')\n",
      "loss pg:  tensor(0.0426, device='mps:0')\n",
      "loss ppo:  tensor(-12.7330, device='mps:0')\n",
      "loss casc:  tensor(-48.9363, device='mps:0')\n",
      "loss pg:  tensor(0.0888, device='mps:0')\n",
      "loss ppo:  tensor(-14.8307, device='mps:0')\n",
      "loss casc:  tensor(-57.4576, device='mps:0')\n",
      "loss pg:  tensor(0.0240, device='mps:0')\n",
      "loss ppo:  tensor(-13.5289, device='mps:0')\n",
      "loss casc:  tensor(-52.9543, device='mps:0')\n",
      "loss pg:  tensor(-0.0468, device='mps:0')\n",
      "loss ppo:  tensor(-14.1228, device='mps:0')\n",
      "loss casc:  tensor(-54.1760, device='mps:0')\n",
      "loss pg:  tensor(-0.1256, device='mps:0')\n",
      "loss ppo:  tensor(-13.6173, device='mps:0')\n",
      "loss casc:  tensor(-52.8457, device='mps:0')\n",
      "loss pg:  tensor(0.0621, device='mps:0')\n",
      "loss ppo:  tensor(-15.0852, device='mps:0')\n",
      "loss casc:  tensor(-57.9722, device='mps:0')\n",
      "loss pg:  tensor(0.0882, device='mps:0')\n",
      "loss ppo:  tensor(-16.3323, device='mps:0')\n",
      "loss casc:  tensor(-63.1024, device='mps:0')\n",
      "loss pg:  tensor(0.0212, device='mps:0')\n",
      "loss ppo:  tensor(-17.1358, device='mps:0')\n",
      "loss casc:  tensor(-67.1704, device='mps:0')\n",
      "loss pg:  tensor(0.0618, device='mps:0')\n",
      "loss ppo:  tensor(-19.8357, device='mps:0')\n",
      "loss casc:  tensor(-76.6790, device='mps:0')\n",
      "loss pg:  tensor(-0.0926, device='mps:0')\n",
      "loss ppo:  tensor(-20.2044, device='mps:0')\n",
      "loss casc:  tensor(-77.4803, device='mps:0')\n",
      "loss pg:  tensor(0.1409, device='mps:0')\n",
      "loss ppo:  tensor(-21.6688, device='mps:0')\n",
      "loss casc:  tensor(-83.7925, device='mps:0')\n",
      "loss pg:  tensor(-0.0778, device='mps:0')\n",
      "loss ppo:  tensor(-19.4870, device='mps:0')\n",
      "loss casc:  tensor(-75.1573, device='mps:0')\n",
      "loss pg:  tensor(-0.0563, device='mps:0')\n",
      "loss ppo:  tensor(-26.1262, device='mps:0')\n",
      "loss casc:  tensor(-101.1576, device='mps:0')\n",
      "loss pg:  tensor(-0.1432, device='mps:0')\n",
      "loss ppo:  tensor(-23.3307, device='mps:0')\n",
      "loss casc:  tensor(-89.4285, device='mps:0')\n",
      "loss pg:  tensor(0.1260, device='mps:0')\n",
      "loss ppo:  tensor(-21.2173, device='mps:0')\n",
      "loss casc:  tensor(-82.6075, device='mps:0')\n",
      "loss pg:  tensor(-0.0023, device='mps:0')\n",
      "loss ppo:  tensor(-25.6108, device='mps:0')\n",
      "loss casc:  tensor(-98.3465, device='mps:0')\n",
      "loss pg:  tensor(0.0607, device='mps:0')\n",
      "loss ppo:  tensor(-26.3258, device='mps:0')\n",
      "loss casc:  tensor(-101.1317, device='mps:0')\n",
      "loss pg:  tensor(-0.1192, device='mps:0')\n",
      "loss ppo:  tensor(-23.4730, device='mps:0')\n",
      "loss casc:  tensor(-90.2975, device='mps:0')\n",
      "loss pg:  tensor(-0.2929, device='mps:0')\n",
      "loss ppo:  tensor(-28.9348, device='mps:0')\n",
      "loss casc:  tensor(-112.2258, device='mps:0')\n",
      "loss pg:  tensor(-0.1310, device='mps:0')\n",
      "loss ppo:  tensor(-30.8160, device='mps:0')\n",
      "loss casc:  tensor(-119.3627, device='mps:0')\n",
      "loss pg:  tensor(-0.1838, device='mps:0')\n",
      "loss ppo:  tensor(-28.1339, device='mps:0')\n",
      "loss casc:  tensor(-108.7623, device='mps:0')\n",
      "loss pg:  tensor(0.0595, device='mps:0')\n",
      "loss ppo:  tensor(-31.6653, device='mps:0')\n",
      "loss casc:  tensor(-120.2657, device='mps:0')\n",
      "loss pg:  tensor(-0.0538, device='mps:0')\n",
      "loss ppo:  tensor(-30.6823, device='mps:0')\n",
      "loss casc:  tensor(-117.8813, device='mps:0')\n",
      "loss pg:  tensor(0.2588, device='mps:0')\n",
      "loss ppo:  tensor(-33.1034, device='mps:0')\n",
      "loss casc:  tensor(-126.4678, device='mps:0')\n",
      "loss pg:  tensor(0.0970, device='mps:0')\n",
      "loss ppo:  tensor(-32.3607, device='mps:0')\n",
      "loss casc:  tensor(-126.1405, device='mps:0')\n",
      "loss pg:  tensor(-0.0537, device='mps:0')\n",
      "loss ppo:  tensor(-38.3833, device='mps:0')\n",
      "loss casc:  tensor(-144.5656, device='mps:0')\n",
      "loss pg:  tensor(0.6583, device='mps:0')\n",
      "loss ppo:  tensor(-39.0668, device='mps:0')\n",
      "loss casc:  tensor(-150.4911, device='mps:0')\n",
      "loss pg:  tensor(0.0391, device='mps:0')\n",
      "loss ppo:  tensor(-38.6941, device='mps:0')\n",
      "loss casc:  tensor(-148.1862, device='mps:0')\n",
      "loss pg:  tensor(0.0732, device='mps:0')\n",
      "loss ppo:  tensor(-37.3620, device='mps:0')\n",
      "loss casc:  tensor(-142.8474, device='mps:0')\n",
      "loss pg:  tensor(0.0968, device='mps:0')\n",
      "loss ppo:  tensor(-39.2073, device='mps:0')\n",
      "loss casc:  tensor(-150.7602, device='mps:0')\n",
      "loss pg:  tensor(-0.5203, device='mps:0')\n",
      "loss ppo:  tensor(-40.2872, device='mps:0')\n",
      "loss casc:  tensor(-153.7695, device='mps:0')\n",
      "loss pg:  tensor(-0.2186, device='mps:0')\n",
      "loss ppo:  tensor(-35.9649, device='mps:0')\n",
      "loss casc:  tensor(-138.0587, device='mps:0')\n",
      "loss pg:  tensor(0.4724, device='mps:0')\n",
      "loss ppo:  tensor(-43.9316, device='mps:0')\n",
      "loss casc:  tensor(-167.7337, device='mps:0')\n",
      "loss pg:  tensor(-0.1411, device='mps:0')\n",
      "loss ppo:  tensor(-41.1216, device='mps:0')\n",
      "loss casc:  tensor(-160.0786, device='mps:0')\n",
      "loss pg:  tensor(0.0988, device='mps:0')\n",
      "loss ppo:  tensor(-44.0876, device='mps:0')\n",
      "loss casc:  tensor(-167.7878, device='mps:0')\n",
      "loss pg:  tensor(0.1689, device='mps:0')\n",
      "loss ppo:  tensor(-40.9025, device='mps:0')\n",
      "loss casc:  tensor(-156.7293, device='mps:0')\n",
      "loss pg:  tensor(-0.7981, device='mps:0')\n",
      "loss ppo:  tensor(-47.5946, device='mps:0')\n",
      "loss casc:  tensor(-181.3508, device='mps:0')\n",
      "loss pg:  tensor(0.2669, device='mps:0')\n",
      "loss ppo:  tensor(-50.0753, device='mps:0')\n",
      "loss casc:  tensor(-189.6788, device='mps:0')\n",
      "loss pg:  tensor(0.0012, device='mps:0')\n",
      "loss ppo:  tensor(-46.8195, device='mps:0')\n",
      "loss casc:  tensor(-178.6105, device='mps:0')\n",
      "loss pg:  tensor(-0.0999, device='mps:0')\n",
      "loss ppo:  tensor(-52.5163, device='mps:0')\n",
      "loss casc:  tensor(-200.2276, device='mps:0')\n",
      "loss pg:  tensor(1.4087, device='mps:0')\n",
      "loss ppo:  tensor(-48.4816, device='mps:0')\n",
      "loss casc:  tensor(-184.7076, device='mps:0')\n",
      "loss pg:  tensor(-0.3148, device='mps:0')\n",
      "loss ppo:  tensor(-48.1529, device='mps:0')\n",
      "loss casc:  tensor(-182.1502, device='mps:0')\n",
      "loss pg:  tensor(0.0639, device='mps:0')\n",
      "loss ppo:  tensor(-50.7784, device='mps:0')\n",
      "loss casc:  tensor(-193.4594, device='mps:0')\n",
      "loss pg:  tensor(-0.2871, device='mps:0')\n",
      "loss ppo:  tensor(-59.6793, device='mps:0')\n",
      "loss casc:  tensor(-223.2209, device='mps:0')\n",
      "loss pg:  tensor(0.2864, device='mps:0')\n",
      "loss ppo:  tensor(-50.9571, device='mps:0')\n",
      "loss casc:  tensor(-195.1024, device='mps:0')\n",
      "loss pg:  tensor(0.2113, device='mps:0')\n",
      "loss ppo:  tensor(-50.8428, device='mps:0')\n",
      "loss casc:  tensor(-191.4125, device='mps:0')\n",
      "loss pg:  tensor(-0.0949, device='mps:0')\n",
      "loss ppo:  tensor(-54.9049, device='mps:0')\n",
      "loss casc:  tensor(-207.6102, device='mps:0')\n",
      "loss pg:  tensor(-0.0625, device='mps:0')\n",
      "loss ppo:  tensor(-51.5557, device='mps:0')\n",
      "loss casc:  tensor(-195.0632, device='mps:0')\n",
      "loss pg:  tensor(-0.1651, device='mps:0')\n",
      "loss ppo:  tensor(-51.0029, device='mps:0')\n",
      "loss casc:  tensor(-193.8718, device='mps:0')\n",
      "loss pg:  tensor(0.0962, device='mps:0')\n",
      "loss ppo:  tensor(-54.2856, device='mps:0')\n",
      "loss casc:  tensor(-202.8310, device='mps:0')\n",
      "loss pg:  tensor(-0.7322, device='mps:0')\n",
      "loss ppo:  tensor(-52.4299, device='mps:0')\n",
      "loss casc:  tensor(-199.7693, device='mps:0')\n",
      "loss pg:  tensor(0.0415, device='mps:0')\n",
      "loss ppo:  tensor(-51.1500, device='mps:0')\n",
      "loss casc:  tensor(-190.2785, device='mps:0')\n",
      "loss pg:  tensor(-0.0144, device='mps:0')\n",
      "loss ppo:  tensor(-55.9450, device='mps:0')\n",
      "loss casc:  tensor(-208.9581, device='mps:0')\n",
      "loss pg:  tensor(1.0185, device='mps:0')\n",
      "loss ppo:  tensor(-53.8431, device='mps:0')\n",
      "loss casc:  tensor(-201.3494, device='mps:0')\n",
      "loss pg:  tensor(0.4481, device='mps:0')\n",
      "loss ppo:  tensor(-57.9964, device='mps:0')\n",
      "loss casc:  tensor(-217.4968, device='mps:0')\n",
      "loss pg:  tensor(0.0600, device='mps:0')\n",
      "loss ppo:  tensor(-53.4360, device='mps:0')\n",
      "loss casc:  tensor(-198.0923, device='mps:0')\n",
      "loss pg:  tensor(0.0138, device='mps:0')\n",
      "loss ppo:  tensor(-52.7906, device='mps:0')\n",
      "loss casc:  tensor(-199.3325, device='mps:0')\n",
      "loss pg:  tensor(-0.1010, device='mps:0')\n",
      "loss ppo:  tensor(-60.1089, device='mps:0')\n",
      "loss casc:  tensor(-224.2137, device='mps:0')\n",
      "loss pg:  tensor(-0.0359, device='mps:0')\n",
      "loss ppo:  tensor(-55.3074, device='mps:0')\n",
      "loss casc:  tensor(-204.5385, device='mps:0')\n",
      "loss pg:  tensor(0.7012, device='mps:0')\n",
      "loss ppo:  tensor(-53.5306, device='mps:0')\n",
      "loss casc:  tensor(-199.0139, device='mps:0')\n",
      "loss pg:  tensor(0.1256, device='mps:0')\n",
      "loss ppo:  tensor(-56.4289, device='mps:0')\n",
      "loss casc:  tensor(-209.7019, device='mps:0')\n",
      "loss pg:  tensor(-0.0145, device='mps:0')\n",
      "loss ppo:  tensor(-57.3787, device='mps:0')\n",
      "loss casc:  tensor(-213.8938, device='mps:0')\n",
      "loss pg:  tensor(-0.0082, device='mps:0')\n",
      "loss ppo:  tensor(-57.2560, device='mps:0')\n",
      "loss casc:  tensor(-209.5274, device='mps:0')\n",
      "loss pg:  tensor(-0.0020, device='mps:0')\n",
      "loss ppo:  tensor(-54.3657, device='mps:0')\n",
      "loss casc:  tensor(-200.4605, device='mps:0')\n",
      "loss pg:  tensor(-0.5058, device='mps:0')\n",
      "loss ppo:  tensor(-54.1355, device='mps:0')\n",
      "loss casc:  tensor(-199.4480, device='mps:0')\n",
      "loss pg:  tensor(-0.4686, device='mps:0')\n",
      "loss ppo:  tensor(-55.0320, device='mps:0')\n",
      "loss casc:  tensor(-202.2864, device='mps:0')\n",
      "loss pg:  tensor(0.0007, device='mps:0')\n",
      "loss ppo:  tensor(-60.1538, device='mps:0')\n",
      "loss casc:  tensor(-224.2471, device='mps:0')\n",
      "loss pg:  tensor(-0.0126, device='mps:0')\n",
      "loss ppo:  tensor(-57.6435, device='mps:0')\n",
      "loss casc:  tensor(-212.5497, device='mps:0')\n",
      "loss pg:  tensor(-0.0061, device='mps:0')\n",
      "loss ppo:  tensor(-57.4780, device='mps:0')\n",
      "loss casc:  tensor(-210.8468, device='mps:0')\n",
      "loss pg:  tensor(0.2204, device='mps:0')\n",
      "loss ppo:  tensor(-54.8904, device='mps:0')\n",
      "loss casc:  tensor(-200.7075, device='mps:0')\n",
      "loss pg:  tensor(0.0302, device='mps:0')\n",
      "loss ppo:  tensor(-57.1355, device='mps:0')\n",
      "loss casc:  tensor(-206.9649, device='mps:0')\n",
      "loss pg:  tensor(0.0007, device='mps:0')\n",
      "loss ppo:  tensor(-63.3274, device='mps:0')\n",
      "loss casc:  tensor(-232.6684, device='mps:0')\n",
      "loss pg:  tensor(-0.0006, device='mps:0')\n",
      "loss ppo:  tensor(-61.6829, device='mps:0')\n",
      "loss casc:  tensor(-222.5499, device='mps:0')\n",
      "[avg actor loss]: -187.5146026611328 \t[critic loss]: 908.4030151367188\n",
      "[train]: epoch n 1\n",
      "loss pg:  tensor(0.0025, device='mps:0')\n",
      "loss ppo:  tensor(-62.8240, device='mps:0')\n",
      "loss casc:  tensor(-229.2035, device='mps:0')\n",
      "loss pg:  tensor(0.0047, device='mps:0')\n",
      "loss ppo:  tensor(-57.6303, device='mps:0')\n",
      "loss casc:  tensor(-208.9068, device='mps:0')\n",
      "loss pg:  tensor(0.0380, device='mps:0')\n",
      "loss ppo:  tensor(-58.2397, device='mps:0')\n",
      "loss casc:  tensor(-211.9033, device='mps:0')\n",
      "loss pg:  tensor(0.2730, device='mps:0')\n",
      "loss ppo:  tensor(-54.8951, device='mps:0')\n",
      "loss casc:  tensor(-199.2723, device='mps:0')\n",
      "loss pg:  tensor(-0.0172, device='mps:0')\n",
      "loss ppo:  tensor(-58.2302, device='mps:0')\n",
      "loss casc:  tensor(-211.0975, device='mps:0')\n",
      "loss pg:  tensor(-0.0056, device='mps:0')\n",
      "loss ppo:  tensor(-58.2916, device='mps:0')\n",
      "loss casc:  tensor(-211.2080, device='mps:0')\n",
      "loss pg:  tensor(-0.0215, device='mps:0')\n",
      "loss ppo:  tensor(-58.7381, device='mps:0')\n",
      "loss casc:  tensor(-210.8170, device='mps:0')\n",
      "loss pg:  tensor(-6.9380, device='mps:0')\n",
      "loss ppo:  tensor(-58.8276, device='mps:0')\n",
      "loss casc:  tensor(-210.6969, device='mps:0')\n",
      "loss pg:  tensor(0.0242, device='mps:0')\n",
      "loss ppo:  tensor(-60.4638, device='mps:0')\n",
      "loss casc:  tensor(-214.7230, device='mps:0')\n",
      "loss pg:  tensor(0.0036, device='mps:0')\n",
      "loss ppo:  tensor(-62.3055, device='mps:0')\n",
      "loss casc:  tensor(-224.7282, device='mps:0')\n",
      "loss pg:  tensor(-0.0075, device='mps:0')\n",
      "loss ppo:  tensor(-60.0494, device='mps:0')\n",
      "loss casc:  tensor(-215.8123, device='mps:0')\n",
      "loss pg:  tensor(0.0564, device='mps:0')\n",
      "loss ppo:  tensor(-58.9743, device='mps:0')\n",
      "loss casc:  tensor(-211.1404, device='mps:0')\n",
      "loss pg:  tensor(-0.0037, device='mps:0')\n",
      "loss ppo:  tensor(-58.9971, device='mps:0')\n",
      "loss casc:  tensor(-211.2941, device='mps:0')\n",
      "loss pg:  tensor(0.0008, device='mps:0')\n",
      "loss ppo:  tensor(-57.9429, device='mps:0')\n",
      "loss casc:  tensor(-208.3453, device='mps:0')\n",
      "loss pg:  tensor(0.0123, device='mps:0')\n",
      "loss ppo:  tensor(-58.2128, device='mps:0')\n",
      "loss casc:  tensor(-210.5108, device='mps:0')\n",
      "loss pg:  tensor(-0.0003, device='mps:0')\n",
      "loss ppo:  tensor(-61.8984, device='mps:0')\n",
      "loss casc:  tensor(-219.3764, device='mps:0')\n",
      "loss pg:  tensor(0.0007, device='mps:0')\n",
      "loss ppo:  tensor(-64.4929, device='mps:0')\n",
      "loss casc:  tensor(-228.1951, device='mps:0')\n",
      "loss pg:  tensor(0.0066, device='mps:0')\n",
      "loss ppo:  tensor(-59.3309, device='mps:0')\n",
      "loss casc:  tensor(-212.5885, device='mps:0')\n",
      "loss pg:  tensor(-0.0169, device='mps:0')\n",
      "loss ppo:  tensor(-60.0821, device='mps:0')\n",
      "loss casc:  tensor(-214.8554, device='mps:0')\n",
      "loss pg:  tensor(-0.0704, device='mps:0')\n",
      "loss ppo:  tensor(-59.4804, device='mps:0')\n",
      "loss casc:  tensor(-209.9730, device='mps:0')\n",
      "loss pg:  tensor(0.0326, device='mps:0')\n",
      "loss ppo:  tensor(-57.8885, device='mps:0')\n",
      "loss casc:  tensor(-205.8266, device='mps:0')\n",
      "loss pg:  tensor(-0.0760, device='mps:0')\n",
      "loss ppo:  tensor(-58.6054, device='mps:0')\n",
      "loss casc:  tensor(-208.4376, device='mps:0')\n",
      "loss pg:  tensor(0.0080, device='mps:0')\n",
      "loss ppo:  tensor(-58.8312, device='mps:0')\n",
      "loss casc:  tensor(-209.8457, device='mps:0')\n",
      "loss pg:  tensor(0.0404, device='mps:0')\n",
      "loss ppo:  tensor(-60.4935, device='mps:0')\n",
      "loss casc:  tensor(-214.3907, device='mps:0')\n",
      "loss pg:  tensor(-0.0047, device='mps:0')\n",
      "loss ppo:  tensor(-59.5978, device='mps:0')\n",
      "loss casc:  tensor(-211.5872, device='mps:0')\n",
      "loss pg:  tensor(0.4454, device='mps:0')\n",
      "loss ppo:  tensor(-60.0556, device='mps:0')\n",
      "loss casc:  tensor(-212.6969, device='mps:0')\n",
      "loss pg:  tensor(-0.9345, device='mps:0')\n",
      "loss ppo:  tensor(-63.4110, device='mps:0')\n",
      "loss casc:  tensor(-225.3797, device='mps:0')\n",
      "loss pg:  tensor(-0.0014, device='mps:0')\n",
      "loss ppo:  tensor(-59.4021, device='mps:0')\n",
      "loss casc:  tensor(-211.3860, device='mps:0')\n",
      "loss pg:  tensor(0.0069, device='mps:0')\n",
      "loss ppo:  tensor(-57.8737, device='mps:0')\n",
      "loss casc:  tensor(-203.1791, device='mps:0')\n",
      "loss pg:  tensor(0.0022, device='mps:0')\n",
      "loss ppo:  tensor(-58.3944, device='mps:0')\n",
      "loss casc:  tensor(-208.4468, device='mps:0')\n",
      "loss pg:  tensor(-0.0030, device='mps:0')\n",
      "loss ppo:  tensor(-61.6390, device='mps:0')\n",
      "loss casc:  tensor(-216.6727, device='mps:0')\n",
      "loss pg:  tensor(0.0024, device='mps:0')\n",
      "loss ppo:  tensor(-58.4262, device='mps:0')\n",
      "loss casc:  tensor(-208.0008, device='mps:0')\n",
      "loss pg:  tensor(-0.0134, device='mps:0')\n",
      "loss ppo:  tensor(-58.3526, device='mps:0')\n",
      "loss casc:  tensor(-209.5139, device='mps:0')\n",
      "loss pg:  tensor(0.0046, device='mps:0')\n",
      "loss ppo:  tensor(-59.9665, device='mps:0')\n",
      "loss casc:  tensor(-210.3850, device='mps:0')\n",
      "loss pg:  tensor(0.0578, device='mps:0')\n",
      "loss ppo:  tensor(-61.4419, device='mps:0')\n",
      "loss casc:  tensor(-217.2398, device='mps:0')\n",
      "loss pg:  tensor(-0.0067, device='mps:0')\n",
      "loss ppo:  tensor(-62.6088, device='mps:0')\n",
      "loss casc:  tensor(-221.6808, device='mps:0')\n",
      "loss pg:  tensor(0.0001, device='mps:0')\n",
      "loss ppo:  tensor(-62.3629, device='mps:0')\n",
      "loss casc:  tensor(-218.3339, device='mps:0')\n",
      "loss pg:  tensor(1.5016, device='mps:0')\n",
      "loss ppo:  tensor(-67.7406, device='mps:0')\n",
      "loss casc:  tensor(-237.8460, device='mps:0')\n",
      "loss pg:  tensor(0.0034, device='mps:0')\n",
      "loss ppo:  tensor(-65.3894, device='mps:0')\n",
      "loss casc:  tensor(-227.8470, device='mps:0')\n",
      "loss pg:  tensor(-0.0022, device='mps:0')\n",
      "loss ppo:  tensor(-59.6119, device='mps:0')\n",
      "loss casc:  tensor(-209.6963, device='mps:0')\n",
      "loss pg:  tensor(0.0062, device='mps:0')\n",
      "loss ppo:  tensor(-62.4055, device='mps:0')\n",
      "loss casc:  tensor(-220.7905, device='mps:0')\n",
      "loss pg:  tensor(-0.0010, device='mps:0')\n",
      "loss ppo:  tensor(-63.2193, device='mps:0')\n",
      "loss casc:  tensor(-221.9019, device='mps:0')\n",
      "loss pg:  tensor(0.0028, device='mps:0')\n",
      "loss ppo:  tensor(-63.0406, device='mps:0')\n",
      "loss casc:  tensor(-223.1648, device='mps:0')\n",
      "loss pg:  tensor(-0.0235, device='mps:0')\n",
      "loss ppo:  tensor(-60.3674, device='mps:0')\n",
      "loss casc:  tensor(-211.1383, device='mps:0')\n",
      "loss pg:  tensor(0.0420, device='mps:0')\n",
      "loss ppo:  tensor(-62.9828, device='mps:0')\n",
      "loss casc:  tensor(-222.5248, device='mps:0')\n",
      "loss pg:  tensor(-0.0035, device='mps:0')\n",
      "loss ppo:  tensor(-61.2867, device='mps:0')\n",
      "loss casc:  tensor(-215.2333, device='mps:0')\n",
      "loss pg:  tensor(-0.0019, device='mps:0')\n",
      "loss ppo:  tensor(-61.9170, device='mps:0')\n",
      "loss casc:  tensor(-218.2214, device='mps:0')\n",
      "loss pg:  tensor(-0.0013, device='mps:0')\n",
      "loss ppo:  tensor(-60.8028, device='mps:0')\n",
      "loss casc:  tensor(-215.4840, device='mps:0')\n",
      "loss pg:  tensor(0.0067, device='mps:0')\n",
      "loss ppo:  tensor(-59.8705, device='mps:0')\n",
      "loss casc:  tensor(-211.3557, device='mps:0')\n",
      "loss pg:  tensor(0.0031, device='mps:0')\n",
      "loss ppo:  tensor(-59.9327, device='mps:0')\n",
      "loss casc:  tensor(-210.9486, device='mps:0')\n",
      "loss pg:  tensor(0.0036, device='mps:0')\n",
      "loss ppo:  tensor(-60.9777, device='mps:0')\n",
      "loss casc:  tensor(-214.1781, device='mps:0')\n",
      "loss pg:  tensor(0.0143, device='mps:0')\n",
      "loss ppo:  tensor(-62.2142, device='mps:0')\n",
      "loss casc:  tensor(-218.7113, device='mps:0')\n",
      "loss pg:  tensor(-0.1540, device='mps:0')\n",
      "loss ppo:  tensor(-62.2218, device='mps:0')\n",
      "loss casc:  tensor(-220.1434, device='mps:0')\n",
      "loss pg:  tensor(0.0020, device='mps:0')\n",
      "loss ppo:  tensor(-58.9726, device='mps:0')\n",
      "loss casc:  tensor(-207.9459, device='mps:0')\n",
      "loss pg:  tensor(0.0099, device='mps:0')\n",
      "loss ppo:  tensor(-62.5986, device='mps:0')\n",
      "loss casc:  tensor(-220.8546, device='mps:0')\n",
      "loss pg:  tensor(0.0007, device='mps:0')\n",
      "loss ppo:  tensor(-65.7962, device='mps:0')\n",
      "loss casc:  tensor(-229.3229, device='mps:0')\n",
      "loss pg:  tensor(-0.0012, device='mps:0')\n",
      "loss ppo:  tensor(-62.7347, device='mps:0')\n",
      "loss casc:  tensor(-219.9627, device='mps:0')\n",
      "loss pg:  tensor(0.0008, device='mps:0')\n",
      "loss ppo:  tensor(-61.5120, device='mps:0')\n",
      "loss casc:  tensor(-217.8726, device='mps:0')\n",
      "loss pg:  tensor(0.0070, device='mps:0')\n",
      "loss ppo:  tensor(-60.2382, device='mps:0')\n",
      "loss casc:  tensor(-210.1723, device='mps:0')\n",
      "loss pg:  tensor(-0.0024, device='mps:0')\n",
      "loss ppo:  tensor(-65.4313, device='mps:0')\n",
      "loss casc:  tensor(-230.6036, device='mps:0')\n",
      "loss pg:  tensor(-0.0024, device='mps:0')\n",
      "loss ppo:  tensor(-64.8299, device='mps:0')\n",
      "loss casc:  tensor(-230.7424, device='mps:0')\n",
      "loss pg:  tensor(0.0003, device='mps:0')\n",
      "loss ppo:  tensor(-62.0925, device='mps:0')\n",
      "loss casc:  tensor(-218.4816, device='mps:0')\n",
      "loss pg:  tensor(0.0002, device='mps:0')\n",
      "loss ppo:  tensor(-59.7808, device='mps:0')\n",
      "loss casc:  tensor(-209.6894, device='mps:0')\n",
      "loss pg:  tensor(-8.4698e-05, device='mps:0')\n",
      "loss ppo:  tensor(-62.4197, device='mps:0')\n",
      "loss casc:  tensor(-219.0385, device='mps:0')\n",
      "loss pg:  tensor(-0.0016, device='mps:0')\n",
      "loss ppo:  tensor(-64.2095, device='mps:0')\n",
      "loss casc:  tensor(-227.7118, device='mps:0')\n",
      "loss pg:  tensor(1.2571e-06, device='mps:0')\n",
      "loss ppo:  tensor(-65.1403, device='mps:0')\n",
      "loss casc:  tensor(-226.8591, device='mps:0')\n",
      "loss pg:  tensor(-0.0005, device='mps:0')\n",
      "loss ppo:  tensor(-63.7757, device='mps:0')\n",
      "loss casc:  tensor(-221.4923, device='mps:0')\n",
      "loss pg:  tensor(0.0010, device='mps:0')\n",
      "loss ppo:  tensor(-60.6263, device='mps:0')\n",
      "loss casc:  tensor(-213.5259, device='mps:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#ppo.load_state_dict(torch.load(\"final_pc.pt\"))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mppo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model_pc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ppo.save_parameters(\u001b[33m\"\u001b[39m\u001b[33mfinal_pc.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 246\u001b[39m, in \u001b[36mPPO.train_model_pc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.critic.optimizer.zero_grad()\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# returns a loss for every net\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m loss_ppo = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_kl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_new_log_policy_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_log_policy_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantage_mini\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m loss_value = \u001b[38;5;28mself\u001b[39m.loss_value(new_value_mini, target_mini)\n\u001b[32m    249\u001b[39m avg_loss_ppo += loss_ppo\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 314\u001b[39m, in \u001b[36mPPO.loss_kl\u001b[39m\u001b[34m(self, stack_new, stack_old, advantage_mini)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# print(\"stack_new\", torch.t(stack_new))\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# print(\"stack_old\", torch.t(stack_old))\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# print(\"stack_new_sliced\", torch.t(stack_new)[:, 1:])\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# print(\"stack_old_sliced\", torch.t(stack_old)[:, 0:stack_old.shape[0]-1])\u001b[39;00m\n\u001b[32m    313\u001b[39m L_casc_init = \u001b[38;5;28mself\u001b[39m.omega12 * (torch.t(stack_new)[:, \u001b[32m1\u001b[39m] - torch.t(stack_old)[:, \u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m kl_sub_previous = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack_new\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m - torch.t(stack_old)[:, \u001b[32m0\u001b[39m:stack_old.shape[\u001b[32m0\u001b[39m]-\u001b[32m1\u001b[39m]\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# I'm appending to the matrix a row which is equal to the last row. At the end i will have a matrix with\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# We also don't need the first two columns of old\u001b[39;00m\n\u001b[32m    317\u001b[39m kl_sub_successive = torch.t(stack_new)[:, \u001b[32m1\u001b[39m:] - torch.t(torch.cat((stack_old, stack_new[stack_new.shape[\u001b[32m0\u001b[39m]-\u001b[32m1\u001b[39m, :].unsqueeze(\u001b[32m0\u001b[39m)), \u001b[32m0\u001b[39m))[:, \u001b[32m2\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/torch/fx/traceback.py:278\u001b[39m, in \u001b[36mformat_stack\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta.get(\u001b[33m\"\u001b[39m\u001b[33mstack_trace\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/traceback.py:45\u001b[39m, in \u001b[36mformat_list\u001b[39m\u001b[34m(extracted_list)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_list\u001b[39m(extracted_list):\n\u001b[32m     34\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format a list of tuples or FrameSummary objects for printing.\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[33;03m    Given a list of tuples or FrameSummary objects as returned by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[33;03m    whose source text line is not None.\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStackSummary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/traceback.py:757\u001b[39m, in \u001b[36mStackSummary.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m frame_summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     formatted_frame = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_frame_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolorize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m formatted_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    759\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/traceback.py:555\u001b[39m, in \u001b[36mStackSummary.format_frame_summary\u001b[39m\u001b[34m(self, frame_summary, **kwargs)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_summary._dedented_lines \u001b[38;5;129;01mand\u001b[39;00m frame_summary._dedented_lines.strip():\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    551\u001b[39m         frame_summary.colno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    552\u001b[39m         frame_summary.end_colno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    553\u001b[39m     ):\n\u001b[32m    554\u001b[39m         \u001b[38;5;66;03m# only output first line if column information is missing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m         row.append(\u001b[43mtextwrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m    \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    557\u001b[39m         \u001b[38;5;66;03m# get first and last line\u001b[39;00m\n\u001b[32m    558\u001b[39m         all_lines_original = frame_summary._original_lines.splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/textwrap.py:486\u001b[39m, in \u001b[36mindent\u001b[39m\u001b[34m(text, prefix, predicate)\u001b[39m\n\u001b[32m    483\u001b[39m     predicate = \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;129;01mnot\u001b[39;00m s.isspace()\n\u001b[32m    485\u001b[39m prefixed_lines = []\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m predicate(line):\n\u001b[32m    488\u001b[39m         prefixed_lines.append(prefix)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#ppo.load_state_dict(torch.load(\"final_pc.pt\"))\n",
    "ppo.train_model_pc()\n",
    "ppo.save_parameters(\"final_pc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a26235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep n 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:39:33.937 Python[17355:9387308] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2026-02-08 15:39:33.937 Python[17355:9387308] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_17355/1076724248.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep n 1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m s = torch.tensor(s, dtype=torch.float32)\n\u001b[32m     39\u001b[39m z, mu, std = ppo.actor_list[\u001b[32m0\u001b[39m](s)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m s, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m s = torch.tensor(s, dtype=torch.float32)\n\u001b[32m     42\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/half_cheetah_v5.py:236\u001b[39m, in \u001b[36mHalfCheetahEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    233\u001b[39m info = {\u001b[33m\"\u001b[39m\u001b[33mx_position\u001b[39m\u001b[33m\"\u001b[39m: x_position_after, \u001b[33m\"\u001b[39m\u001b[33mx_velocity\u001b[39m\u001b[33m\"\u001b[39m: x_velocity, **reward_info}\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_env.py:158\u001b[39m, in \u001b[36mMujocoEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmujoco_renderer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:767\u001b[39m, in \u001b[36mMujocoRenderer.render\u001b[39m\u001b[34m(self, render_mode)\u001b[39m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer.render(render_mode=render_mode, camera_id=\u001b[38;5;28mself\u001b[39m.camera_id)\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:489\u001b[39m, in \u001b[36mWindowViewer.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[38;5;28mself\u001b[39m._loop_count = \u001b[32m1\u001b[39m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop_count > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m         \u001b[38;5;28mself\u001b[39m._loop_count -= \u001b[32m1\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Reinforcement Learning/exam project/.venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:461\u001b[39m, in \u001b[36mWindowViewer.render.<locals>.update\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hide_menu:\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gridpos, [t1, t2] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._overlays.items():\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m         \u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjr_overlay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjtFontScale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjFONTSCALE_150\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgridpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m glfw.swap_buffers(\u001b[38;5;28mself\u001b[39m.window)\n\u001b[32m    471\u001b[39m glfw.poll_events()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi  verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "ppo = PPO(epochs=epochs, \n",
    "          training_iterations=training_iterations,\n",
    "          batch_size=batch_size,\n",
    "          trajectory_length=trajectory_length, \n",
    "          n_actors=n_actors,\n",
    "          env=env,\n",
    "          in_features=in_features,\n",
    "          out_features=out_features,\n",
    "          hidden_features=hidden_features,\n",
    "          device=device,\n",
    "          actor_learning_rate=actor_learning_rate,\n",
    "          critic_learning_rate=critic_learning_rate,\n",
    "          gamma=gamma,\n",
    "          lambda_=lambda_,\n",
    "          epsilon=epsilon,\n",
    "          beta = beta,\n",
    "          d_targ=d_targ,\n",
    "          std=std,\n",
    "          mode=mode,\n",
    "          n_nets=n_nets,\n",
    "          omega=omega,\n",
    "          omega12=omega12,\n",
    "        )\n",
    "\n",
    "ppo.load_state_dict(torch.load(\"model12.pt\"))\n",
    "\n",
    "env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1, render_mode=\"human\")\n",
    "#env = gym.make('HalfCheetah-v5', ctrl_cost_weight=0.1)\n",
    "rewards = []\n",
    "for episode in range(10):\n",
    "    print(f\"ep n {episode}\", \"\\r\")\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    s, _ = env.reset()\n",
    "    while not done:\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        z, mu, std = ppo.actor_list[0](s)\n",
    "        s, reward, terminated, truncated, info = env.step(z.squeeze().cpu().detach().numpy())\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
